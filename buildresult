[INFO] Scanning for projects...
[INFO]                                                                         
[INFO] ------------------------------------------------------------------------
[INFO] Building Spark Project Core 1.3.0
[INFO] ------------------------------------------------------------------------
[INFO] 
[INFO] --- maven-enforcer-plugin:1.3.1:enforce (enforce-versions) @ spark-core_2.10 ---
[INFO] 
[INFO] --- scala-maven-plugin:3.2.0:add-source (eclipse-add-source) @ spark-core_2.10 ---
[INFO] Add Source directory: /home/hadoop/develop/spark/core/src/main/scala
[INFO] Add Test Source directory: /home/hadoop/develop/spark/core/src/test/scala
[INFO] 
[INFO] --- build-helper-maven-plugin:1.8:add-source (add-scala-sources) @ spark-core_2.10 ---
[INFO] Source directory: /home/hadoop/develop/spark/core/src/main/scala added.
[INFO] 
[INFO] --- maven-remote-resources-plugin:1.5:process (default) @ spark-core_2.10 ---
[INFO] 
[INFO] --- maven-antrun-plugin:1.7:run (default) @ spark-core_2.10 ---
[INFO] Executing tasks

main:
    [unzip] Expanding: /home/hadoop/develop/spark/python/lib/py4j-0.8.2.1-src.zip into /home/hadoop/develop/spark/python/build
[INFO] Executed tasks
[INFO] 
[INFO] --- maven-resources-plugin:2.6:resources (default-resources) @ spark-core_2.10 ---
[INFO] Using 'UTF-8' encoding to copy filtered resources.
[INFO] Copying 11 resources
[INFO] Copying 24 resources
[INFO] Copying 7 resources
[INFO] Copying 3 resources
[INFO] 
[INFO] --- scala-maven-plugin:3.2.0:compile (scala-compile-first) @ spark-core_2.10 ---
[INFO] Using zinc server for incremental compilation
[INFO] compiler plugin: BasicArtifact(org.scalamacros,paradise_2.10.4,2.0.1,null)
[0m[[0minfo[0m] [0mCompiling 437 Scala sources and 44 Java sources to /home/hadoop/develop/spark/core/target/scala-2.10/classes...[0m
[0m[[33mwarn[0m] [0m/home/hadoop/develop/spark/core/src/main/scala/org/apache/spark/scheduler/local/LocalBackend.scala:89: postfix operator millis should be enabled[0m
[0m[[33mwarn[0m] [0mby making the implicit value scala.language.postfixOps visible.[0m
[0m[[33mwarn[0m] [0mThis can be achieved by adding the import clause 'import scala.language.postfixOps'[0m
[0m[[33mwarn[0m] [0mor by setting the compiler option -language:postfixOps.[0m
[0m[[33mwarn[0m] [0mSee the Scala docs for value scala.language.postfixOps for a discussion[0m
[0m[[33mwarn[0m] [0mwhy the feature should be explicitly enabled.[0m
[0m[[33mwarn[0m] [0m      context.system.scheduler.scheduleOnce(1000 millis, self, ReviveOffers)[0m
[0m[[33mwarn[0m] [0m                                                 ^[0m
[0m[[33mwarn[0m] [0m/home/hadoop/develop/spark/core/src/main/scala/org/apache/spark/util/MutableURLClassLoader.scala:26: imported `ParentClassLoader' is permanently hidden by definition of class ParentClassLoader in package util[0m
[0m[[33mwarn[0m] [0mimport org.apache.spark.util.ParentClassLoader[0m
[0m[[33mwarn[0m] [0m                             ^[0m
[0m[[33mwarn[0m] [0m/home/hadoop/develop/spark/core/src/main/scala/org/apache/spark/TaskState.scala:41: match may not be exhaustive.[0m
[0m[[33mwarn[0m] [0mIt would fail on the following input: TASK_ERROR[0m
[0m[[33mwarn[0m] [0m  def fromMesos(mesosState: MesosTaskState): TaskState = mesosState match {[0m
[0m[[33mwarn[0m] [0m                                                         ^[0m
[0m[[33mwarn[0m] [0m/home/hadoop/develop/spark/core/src/main/scala/org/apache/spark/SparkContext.scala:623: constructor Job in class Job is deprecated: see corresponding Javadoc for more information.[0m
[0m[[33mwarn[0m] [0m    val job = new NewHadoopJob(hadoopConfiguration)[0m
[0m[[33mwarn[0m] [0m              ^[0m
[0m[[33mwarn[0m] [0m/home/hadoop/develop/spark/core/src/main/scala/org/apache/spark/SparkContext.scala:669: constructor Job in class Job is deprecated: see corresponding Javadoc for more information.[0m
[0m[[33mwarn[0m] [0m    val job = new NewHadoopJob(hadoopConfiguration)[0m
[0m[[33mwarn[0m] [0m              ^[0m
[0m[[33mwarn[0m] [0m/home/hadoop/develop/spark/core/src/main/scala/org/apache/spark/SparkContext.scala:845: constructor Job in class Job is deprecated: see corresponding Javadoc for more information.[0m
[0m[[33mwarn[0m] [0m    val job = new NewHadoopJob(conf)[0m
[0m[[33mwarn[0m] [0m              ^[0m
[0m[[33mwarn[0m] [0m/home/hadoop/develop/spark/core/src/main/scala/org/apache/spark/SparkHadoopWriter.scala:201: constructor TaskID in class TaskID is deprecated: see corresponding Javadoc for more information.[0m
[0m[[33mwarn[0m] [0m        new TaskAttemptID(new TaskID(jID.value, true, splitID), attemptID))[0m
[0m[[33mwarn[0m] [0m                          ^[0m
[0m[[33mwarn[0m] [0m/home/hadoop/develop/spark/core/src/main/scala/org/apache/spark/SparkHadoopWriter.scala:222: method makeQualified in class Path is deprecated: see corresponding Javadoc for more information.[0m
[0m[[33mwarn[0m] [0m    outputPath.makeQualified(fs)[0m
[0m[[33mwarn[0m] [0m               ^[0m
[0m[[33mwarn[0m] [0m/home/hadoop/develop/spark/core/src/main/scala/org/apache/spark/deploy/SparkHadoopUtil.scala:197: method isDir in class FileStatus is deprecated: see corresponding Javadoc for more information.[0m
[0m[[33mwarn[0m] [0m      val (directories, leaves) = fs.listStatus(path).partition(_.isDir)[0m
[0m[[33mwarn[0m] [0m                                                                  ^[0m
[0m[[33mwarn[0m] [0m/home/hadoop/develop/spark/core/src/main/scala/org/apache/spark/deploy/SparkHadoopUtil.scala:202: method isDir in class FileStatus is deprecated: see corresponding Javadoc for more information.[0m
[0m[[33mwarn[0m] [0m    if (baseStatus.isDir) recurse(basePath) else Array(baseStatus)[0m
[0m[[33mwarn[0m] [0m                   ^[0m
[0m[[33mwarn[0m] [0m/home/hadoop/develop/spark/core/src/main/scala/org/apache/spark/deploy/history/FsHistoryProvider.scala:111: method isDir in class FileStatus is deprecated: see corresponding Javadoc for more information.[0m
[0m[[33mwarn[0m] [0m    if (!fs.getFileStatus(path).isDir) {[0m
[0m[[33mwarn[0m] [0m                                ^[0m
[0m[[33mwarn[0m] [0m/home/hadoop/develop/spark/core/src/main/scala/org/apache/spark/deploy/history/FsHistoryProvider.scala:318: method isDir in class FileStatus is deprecated: see corresponding Javadoc for more information.[0m
[0m[[33mwarn[0m] [0m  private def isLegacyLogDirectory(entry: FileStatus): Boolean = entry.isDir()[0m
[0m[[33mwarn[0m] [0m                                                                       ^[0m
[0m[[33mwarn[0m] [0m/home/hadoop/develop/spark/core/src/main/scala/org/apache/spark/input/PortableDataStream.scala:49: method isDir in class FileStatus is deprecated: see corresponding Javadoc for more information.[0m
[0m[[33mwarn[0m] [0m      if (file.isDir) 0L else file.getLen[0m
[0m[[33mwarn[0m] [0m               ^[0m
[0m[[33mwarn[0m] [0m/home/hadoop/develop/spark/core/src/main/scala/org/apache/spark/input/WholeTextFileInputFormat.scala:57: method isDir in class FileStatus is deprecated: see corresponding Javadoc for more information.[0m
[0m[[33mwarn[0m] [0m      if (file.isDir) 0L else file.getLen[0m
[0m[[33mwarn[0m] [0m               ^[0m
[0m[[33mwarn[0m] [0m/home/hadoop/develop/spark/core/src/main/scala/org/apache/spark/mapred/SparkHadoopMapRedUtil.scala:56: constructor TaskAttemptID in class TaskAttemptID is deprecated: see corresponding Javadoc for more information.[0m
[0m[[33mwarn[0m] [0m    new TaskAttemptID(jtIdentifier, jobId, isMap, taskId, attemptId)[0m
[0m[[33mwarn[0m] [0m    ^[0m
[0m[[33mwarn[0m] [0m/home/hadoop/develop/spark/core/src/main/scala/org/apache/spark/rdd/CheckpointRDD.scala:111: method getDefaultReplication in class FileSystem is deprecated: see corresponding Javadoc for more information.[0m
[0m[[33mwarn[0m] [0m      fs.create(tempOutputPath, false, bufferSize, fs.getDefaultReplication, blockSize)[0m
[0m[[33mwarn[0m] [0m                                                      ^[0m
[0m[[33mwarn[0m] [0m/home/hadoop/develop/spark/core/src/main/scala/org/apache/spark/rdd/HadoopRDD.scala:351: constructor TaskID in class TaskID is deprecated: see corresponding Javadoc for more information.[0m
[0m[[33mwarn[0m] [0m    val taId = new TaskAttemptID(new TaskID(jobID, true, splitId), attemptId)[0m
[0m[[33mwarn[0m] [0m                                 ^[0m
[0m[[33mwarn[0m] [0m/home/hadoop/develop/spark/core/src/main/scala/org/apache/spark/rdd/PairRDDFunctions.scala:898: constructor Job in class Job is deprecated: see corresponding Javadoc for more information.[0m
[0m[[33mwarn[0m] [0m    val job = new NewAPIHadoopJob(hadoopConf)[0m
[0m[[33mwarn[0m] [0m              ^[0m
[0m[[33mwarn[0m] [0m/home/hadoop/develop/spark/core/src/main/scala/org/apache/spark/rdd/PairRDDFunctions.scala:966: constructor Job in class Job is deprecated: see corresponding Javadoc for more information.[0m
[0m[[33mwarn[0m] [0m    val job = new NewAPIHadoopJob(hadoopConf)[0m
[0m[[33mwarn[0m] [0m              ^[0m
[0m[[33mwarn[0m] [0m/home/hadoop/develop/spark/core/src/main/scala/org/apache/spark/scheduler/InputFormatInfo.scala:106: constructor Job in class Job is deprecated: see corresponding Javadoc for more information.[0m
[0m[[33mwarn[0m] [0m    val job = new Job(conf)[0m
[0m[[33mwarn[0m] [0m              ^[0m
[0m[[33mwarn[0m] [0m/home/hadoop/develop/spark/core/src/main/scala/org/apache/spark/smstorage/worker/ExecutorWatcher.scala:117: comparing values of types Unit and Null using `!=' will always yield true[0m
[0m[[33mwarn[0m] [0m      while ((line = reader.readLine()) != null) {[0m
[0m[[33mwarn[0m] [0m                                        ^[0m
[0m[[33mwarn[0m] [0m21 warnings found[0m
[0m[[33mwarn[0m] [0mwarning: [options] bootstrap class path not set in conjunction with -source 1.6[0m
[0m[[33mwarn[0m] [0m1 warning[0m
[0m[[0minfo[0m] [0mCompile success at May 22, 2016 10:54:46 PM [3:09.794s][0m
[INFO] 
[INFO] --- maven-compiler-plugin:3.1:compile (default-compile) @ spark-core_2.10 ---
[INFO] Changes detected - recompiling the module!
[INFO] Compiling 44 source files to /home/hadoop/develop/spark/core/target/scala-2.10/classes
[INFO] 
[INFO] --- build-helper-maven-plugin:1.8:add-test-source (add-scala-test-sources) @ spark-core_2.10 ---
[INFO] Test Source directory: /home/hadoop/develop/spark/core/src/test/scala added.
[INFO] 
[INFO] --- maven-resources-plugin:2.6:testResources (default-testResources) @ spark-core_2.10 ---
[INFO] Using 'UTF-8' encoding to copy filtered resources.
[INFO] Copying 7 resources
[INFO] Copying 3 resources
[INFO] 
[INFO] --- scala-maven-plugin:3.2.0:testCompile (scala-test-compile-first) @ spark-core_2.10 ---
[INFO] Using zinc server for incremental compilation
[INFO] compiler plugin: BasicArtifact(org.scalamacros,paradise_2.10.4,2.0.1,null)
[0m[[0minfo[0m] [0mCompiling 145 Scala sources and 11 Java sources to /home/hadoop/develop/spark/core/target/scala-2.10/test-classes...[0m
[0m[[33mwarn[0m] [0m/home/hadoop/develop/spark/core/src/test/scala/org/apache/spark/scheduler/OutputCommitCoordinatorSuite.scala:156: a pure expression does nothing in statement position; you may be omitting necessary parentheses[0m
[0m[[33mwarn[0m] [0m      0 until rdd.partitions.size, resultHandler, 0)[0m
[0m[[33mwarn[0m] [0m                                                  ^[0m
[0m[[33mwarn[0m] [0m/home/hadoop/develop/spark/core/src/test/scala/org/apache/spark/util/MutableURLClassLoaderSuite.scala:25: imported `Utils' is permanently hidden by definition of object Utils in package util[0m
[0m[[33mwarn[0m] [0mimport org.apache.spark.util.Utils[0m
[0m[[33mwarn[0m] [0m                             ^[0m
[0m[[33mwarn[0m] [0m/home/hadoop/develop/spark/core/src/test/scala/org/apache/spark/FileSuite.scala:496: constructor Job in class Job is deprecated: see corresponding Javadoc for more information.[0m
[0m[[33mwarn[0m] [0m    val job = new Job(sc.hadoopConfiguration)[0m
[0m[[33mwarn[0m] [0m              ^[0m
[0m[[33mwarn[0m] [0m/home/hadoop/develop/spark/core/src/test/scala/org/apache/spark/scheduler/EventLoggingListenerSuite.scala:69: method isDir in class FileStatus is deprecated: see corresponding Javadoc for more information.[0m
[0m[[33mwarn[0m] [0m    assert(!logStatus.isDir)[0m
[0m[[33mwarn[0m] [0m                      ^[0m
[0m[[33mwarn[0m] [0m/home/hadoop/develop/spark/core/src/test/scala/org/apache/spark/scheduler/EventLoggingListenerSuite.scala:73: method isDir in class FileStatus is deprecated: see corresponding Javadoc for more information.[0m
[0m[[33mwarn[0m] [0m    assert(!fileSystem.getFileStatus(new Path(eventLogger.logPath)).isDir)[0m
[0m[[33mwarn[0m] [0m                                                                    ^[0m
[0m[[33mwarn[0m] [0m/home/hadoop/develop/spark/core/src/test/scala/org/apache/spark/scheduler/ReplayListenerSuite.scala:115: method isDir in class FileStatus is deprecated: see corresponding Javadoc for more information.[0m
[0m[[33mwarn[0m] [0m    assert(!eventLog.isDir)[0m
[0m[[33mwarn[0m] [0m                     ^[0m
[0m[[33mwarn[0m] [0m/home/hadoop/develop/spark/core/src/test/scala/org/apache/spark/scheduler/TaskContextSuite.scala:89: method attemptId in class TaskContext is deprecated: use attemptNumber[0m
[0m[[33mwarn[0m] [0m      Seq(TaskContext.get().attemptId).iterator[0m
[0m[[33mwarn[0m] [0m                            ^[0m
[0m[[33mwarn[0m] [0m7 warnings found[0m
[0m[[33mwarn[0m] [0mwarning: [options] bootstrap class path not set in conjunction with -source 1.6[0m
[0m[[33mwarn[0m] [0mNote: /home/hadoop/develop/spark/core/src/test/java/org/apache/spark/JavaAPISuite.java uses or overrides a deprecated API.[0m
[0m[[33mwarn[0m] [0mNote: Recompile with -Xlint:deprecation for details.[0m
[0m[[33mwarn[0m] [0m1 warning[0m
[0m[[0minfo[0m] [0mCompile success at May 22, 2016 10:56:38 PM [1:44.256s][0m
[INFO] 
[INFO] --- maven-compiler-plugin:3.1:testCompile (default-testCompile) @ spark-core_2.10 ---
[INFO] Nothing to compile - all classes are up to date
[INFO] 
[INFO] --- maven-dependency-plugin:2.9:build-classpath (default) @ spark-core_2.10 ---
[INFO] Skipped writing classpath file '/home/hadoop/develop/spark/core/target/spark-test-classpath.txt'.  No changes found.
[INFO] 
[INFO] --- gmavenplus-plugin:1.2:execute (default) @ spark-core_2.10 ---
[INFO] Using Groovy 2.3.7 to perform execute.
[INFO] 
[INFO] --- maven-surefire-plugin:2.18:test (default-test) @ spark-core_2.10 ---
[INFO] Tests are skipped.
[INFO] 
[INFO] --- scalatest-maven-plugin:1.0:test (test) @ spark-core_2.10 ---
[INFO] Tests are skipped.
[INFO] 
[INFO] --- maven-jar-plugin:2.4:jar (default-jar) @ spark-core_2.10 ---
[INFO] Building jar: /home/hadoop/develop/spark/core/target/spark-core_2.10-1.3.0.jar
[INFO] 
[INFO] --- maven-site-plugin:3.3:attach-descriptor (attach-descriptor) @ spark-core_2.10 ---
[INFO] 
[INFO] --- maven-dependency-plugin:2.9:copy-dependencies (copy-dependencies) @ spark-core_2.10 ---
[INFO] 
[INFO] --- maven-shade-plugin:2.2:shade (default) @ spark-core_2.10 ---
[INFO] Excluding com.twitter:chill_2.10:jar:0.5.0 from the shaded jar.
[INFO] Excluding com.esotericsoftware.kryo:kryo:jar:2.21 from the shaded jar.
[INFO] Excluding com.esotericsoftware.reflectasm:reflectasm:jar:shaded:1.07 from the shaded jar.
[INFO] Excluding com.esotericsoftware.minlog:minlog:jar:1.2 from the shaded jar.
[INFO] Excluding org.objenesis:objenesis:jar:1.2 from the shaded jar.
[INFO] Excluding com.twitter:chill-java:jar:0.5.0 from the shaded jar.
[INFO] Excluding org.apache.hadoop:hadoop-client:jar:2.3.0 from the shaded jar.
[INFO] Excluding org.apache.hadoop:hadoop-common:jar:2.3.0 from the shaded jar.
[INFO] Excluding commons-cli:commons-cli:jar:1.2 from the shaded jar.
[INFO] Excluding xmlenc:xmlenc:jar:0.52 from the shaded jar.
[INFO] Excluding commons-httpclient:commons-httpclient:jar:3.1 from the shaded jar.
[INFO] Excluding commons-io:commons-io:jar:2.4 from the shaded jar.
[INFO] Excluding commons-collections:commons-collections:jar:3.2.1 from the shaded jar.
[INFO] Excluding commons-lang:commons-lang:jar:2.6 from the shaded jar.
[INFO] Excluding commons-configuration:commons-configuration:jar:1.6 from the shaded jar.
[INFO] Excluding commons-digester:commons-digester:jar:1.8 from the shaded jar.
[INFO] Excluding commons-beanutils:commons-beanutils:jar:1.7.0 from the shaded jar.
[INFO] Excluding commons-beanutils:commons-beanutils-core:jar:1.8.0 from the shaded jar.
[INFO] Excluding org.apache.avro:avro:jar:1.7.6 from the shaded jar.
[INFO] Excluding com.google.protobuf:protobuf-java:jar:2.5.0 from the shaded jar.
[INFO] Excluding org.apache.hadoop:hadoop-auth:jar:2.3.0 from the shaded jar.
[INFO] Excluding org.apache.commons:commons-compress:jar:1.4.1 from the shaded jar.
[INFO] Excluding org.tukaani:xz:jar:1.0 from the shaded jar.
[INFO] Excluding org.apache.hadoop:hadoop-hdfs:jar:2.3.0 from the shaded jar.
[INFO] Excluding org.mortbay.jetty:jetty-util:jar:6.1.26 from the shaded jar.
[INFO] Excluding org.apache.hadoop:hadoop-mapreduce-client-app:jar:2.3.0 from the shaded jar.
[INFO] Excluding org.apache.hadoop:hadoop-mapreduce-client-common:jar:2.3.0 from the shaded jar.
[INFO] Excluding org.apache.hadoop:hadoop-yarn-client:jar:2.3.0 from the shaded jar.
[INFO] Excluding org.apache.hadoop:hadoop-yarn-server-common:jar:2.3.0 from the shaded jar.
[INFO] Excluding org.apache.hadoop:hadoop-mapreduce-client-shuffle:jar:2.3.0 from the shaded jar.
[INFO] Excluding org.apache.hadoop:hadoop-yarn-api:jar:2.3.0 from the shaded jar.
[INFO] Excluding org.apache.hadoop:hadoop-mapreduce-client-core:jar:2.3.0 from the shaded jar.
[INFO] Excluding org.apache.hadoop:hadoop-yarn-common:jar:2.3.0 from the shaded jar.
[INFO] Excluding javax.xml.bind:jaxb-api:jar:2.2.2 from the shaded jar.
[INFO] Excluding javax.xml.stream:stax-api:jar:1.0-2 from the shaded jar.
[INFO] Excluding com.sun.jersey:jersey-core:jar:1.9 from the shaded jar.
[INFO] Excluding org.apache.hadoop:hadoop-mapreduce-client-jobclient:jar:2.3.0 from the shaded jar.
[INFO] Excluding org.apache.hadoop:hadoop-annotations:jar:2.3.0 from the shaded jar.
[INFO] Excluding org.apache.spark:spark-network-common_2.10:jar:1.3.0 from the shaded jar.
[INFO] Excluding org.apache.spark:spark-network-shuffle_2.10:jar:1.3.0 from the shaded jar.
[INFO] Excluding net.java.dev.jets3t:jets3t:jar:0.9.3 from the shaded jar.
[INFO] Excluding org.apache.httpcomponents:httpcore:jar:4.3.3 from the shaded jar.
[INFO] Excluding org.apache.httpcomponents:httpclient:jar:4.3.6 from the shaded jar.
[INFO] Excluding commons-codec:commons-codec:jar:1.10 from the shaded jar.
[INFO] Excluding org.codehaus.jackson:jackson-core-asl:jar:1.9.13 from the shaded jar.
[INFO] Excluding org.codehaus.jackson:jackson-mapper-asl:jar:1.9.13 from the shaded jar.
[INFO] Excluding javax.activation:activation:jar:1.1.1 from the shaded jar.
[INFO] Excluding mx4j:mx4j:jar:3.0.2 from the shaded jar.
[INFO] Excluding javax.mail:mail:jar:1.4.7 from the shaded jar.
[INFO] Excluding org.bouncycastle:bcprov-jdk15on:jar:1.51 from the shaded jar.
[INFO] Excluding com.jamesmurty.utils:java-xmlbuilder:jar:1.0 from the shaded jar.
[INFO] Excluding net.iharder:base64:jar:2.3.8 from the shaded jar.
[INFO] Excluding org.apache.curator:curator-recipes:jar:2.4.0 from the shaded jar.
[INFO] Excluding org.apache.curator:curator-framework:jar:2.4.0 from the shaded jar.
[INFO] Excluding org.apache.curator:curator-client:jar:2.4.0 from the shaded jar.
[INFO] Excluding org.apache.zookeeper:zookeeper:jar:3.4.5 from the shaded jar.
[INFO] Excluding jline:jline:jar:0.9.94 from the shaded jar.
[INFO] Including org.eclipse.jetty:jetty-plus:jar:8.1.14.v20131031 in the shaded jar.
[INFO] Excluding org.eclipse.jetty.orbit:javax.transaction:jar:1.1.1.v201105210645 from the shaded jar.
[INFO] Excluding org.eclipse.jetty:jetty-webapp:jar:8.1.14.v20131031 from the shaded jar.
[INFO] Excluding org.eclipse.jetty:jetty-xml:jar:8.1.14.v20131031 from the shaded jar.
[INFO] Excluding org.eclipse.jetty:jetty-jndi:jar:8.1.14.v20131031 from the shaded jar.
[INFO] Excluding org.eclipse.jetty.orbit:javax.mail.glassfish:jar:1.4.1.v201005082020 from the shaded jar.
[INFO] Excluding org.eclipse.jetty.orbit:javax.activation:jar:1.1.0.v201105071233 from the shaded jar.
[INFO] Including org.eclipse.jetty:jetty-security:jar:8.1.14.v20131031 in the shaded jar.
[INFO] Including org.eclipse.jetty:jetty-util:jar:8.1.14.v20131031 in the shaded jar.
[INFO] Including org.eclipse.jetty:jetty-server:jar:8.1.14.v20131031 in the shaded jar.
[INFO] Including org.eclipse.jetty:jetty-http:jar:8.1.14.v20131031 in the shaded jar.
[INFO] Including org.eclipse.jetty:jetty-io:jar:8.1.14.v20131031 in the shaded jar.
[INFO] Including org.eclipse.jetty:jetty-continuation:jar:8.1.14.v20131031 in the shaded jar.
[INFO] Including org.eclipse.jetty:jetty-servlet:jar:8.1.14.v20131031 in the shaded jar.
[INFO] Excluding org.eclipse.jetty.orbit:javax.servlet:jar:3.0.0.v201112011016 from the shaded jar.
[INFO] Excluding org.apache.commons:commons-lang3:jar:3.3.2 from the shaded jar.
[INFO] Excluding org.apache.commons:commons-math3:jar:3.1.1 from the shaded jar.
[INFO] Excluding com.google.code.findbugs:jsr305:jar:1.3.9 from the shaded jar.
[INFO] Excluding org.slf4j:slf4j-api:jar:1.7.10 from the shaded jar.
[INFO] Excluding org.slf4j:jul-to-slf4j:jar:1.7.10 from the shaded jar.
[INFO] Excluding org.slf4j:jcl-over-slf4j:jar:1.7.10 from the shaded jar.
[INFO] Excluding log4j:log4j:jar:1.2.17 from the shaded jar.
[INFO] Excluding org.slf4j:slf4j-log4j12:jar:1.7.10 from the shaded jar.
[INFO] Excluding com.ning:compress-lzf:jar:1.0.0 from the shaded jar.
[INFO] Excluding org.xerial.snappy:snappy-java:jar:1.1.1.6 from the shaded jar.
[INFO] Excluding net.jpountz.lz4:lz4:jar:1.2.0 from the shaded jar.
[INFO] Excluding org.roaringbitmap:RoaringBitmap:jar:0.4.5 from the shaded jar.
[INFO] Excluding commons-net:commons-net:jar:2.2 from the shaded jar.
[INFO] Excluding org.spark-project.akka:akka-remote_2.10:jar:2.3.4-spark from the shaded jar.
[INFO] Excluding org.spark-project.akka:akka-actor_2.10:jar:2.3.4-spark from the shaded jar.
[INFO] Excluding com.typesafe:config:jar:1.2.1 from the shaded jar.
[INFO] Excluding io.netty:netty:jar:3.8.0.Final from the shaded jar.
[INFO] Excluding org.spark-project.protobuf:protobuf-java:jar:2.5.0-spark from the shaded jar.
[INFO] Excluding org.uncommons.maths:uncommons-maths:jar:1.2.2a from the shaded jar.
[INFO] Excluding org.spark-project.akka:akka-slf4j_2.10:jar:2.3.4-spark from the shaded jar.
[INFO] Excluding org.scala-lang:scala-library:jar:2.10.4 from the shaded jar.
[INFO] Excluding org.json4s:json4s-jackson_2.10:jar:3.2.10 from the shaded jar.
[INFO] Excluding org.json4s:json4s-core_2.10:jar:3.2.10 from the shaded jar.
[INFO] Excluding org.json4s:json4s-ast_2.10:jar:3.2.10 from the shaded jar.
[INFO] Excluding org.scala-lang:scalap:jar:2.10.4 from the shaded jar.
[INFO] Excluding org.scala-lang:scala-compiler:jar:2.10.4 from the shaded jar.
[INFO] Excluding org.apache.mesos:mesos:jar:shaded-protobuf:0.21.0 from the shaded jar.
[INFO] Excluding io.netty:netty-all:jar:4.0.23.Final from the shaded jar.
[INFO] Excluding com.clearspring.analytics:stream:jar:2.7.0 from the shaded jar.
[INFO] Excluding io.dropwizard.metrics:metrics-core:jar:3.1.0 from the shaded jar.
[INFO] Excluding io.dropwizard.metrics:metrics-jvm:jar:3.1.0 from the shaded jar.
[INFO] Excluding io.dropwizard.metrics:metrics-json:jar:3.1.0 from the shaded jar.
[INFO] Excluding io.dropwizard.metrics:metrics-graphite:jar:3.1.0 from the shaded jar.
[INFO] Excluding com.fasterxml.jackson.core:jackson-databind:jar:2.4.4 from the shaded jar.
[INFO] Excluding com.fasterxml.jackson.core:jackson-annotations:jar:2.4.0 from the shaded jar.
[INFO] Excluding com.fasterxml.jackson.core:jackson-core:jar:2.4.4 from the shaded jar.
[INFO] Excluding com.fasterxml.jackson.module:jackson-module-scala_2.10:jar:2.4.4 from the shaded jar.
[INFO] Excluding org.scala-lang:scala-reflect:jar:2.10.4 from the shaded jar.
[INFO] Excluding com.thoughtworks.paranamer:paranamer:jar:2.6 from the shaded jar.
[INFO] Excluding org.apache.ivy:ivy:jar:2.4.0 from the shaded jar.
[INFO] Excluding oro:oro:jar:2.0.8 from the shaded jar.
[INFO] Excluding org.tachyonproject:tachyon-client:jar:0.5.0 from the shaded jar.
[INFO] Excluding org.tachyonproject:tachyon:jar:0.5.0 from the shaded jar.
[INFO] Excluding commons-logging:commons-logging:jar:1.1.3 from the shaded jar.
[INFO] Excluding org.spark-project:pyrolite:jar:2.0.1 from the shaded jar.
[INFO] Excluding net.sf.py4j:py4j:jar:0.8.2.1 from the shaded jar.
[INFO] Including org.spark-project.spark:unused:jar:1.0.0 in the shaded jar.
[INFO] Replacing original artifact with shaded artifact.
[INFO] Replacing /home/hadoop/develop/spark/core/target/spark-core_2.10-1.3.0.jar with /home/hadoop/develop/spark/core/target/spark-core_2.10-1.3.0-shaded.jar
[INFO] Dependency-reduced POM written at: /home/hadoop/develop/spark/core/dependency-reduced-pom.xml
[INFO] Dependency-reduced POM written at: /home/hadoop/develop/spark/core/dependency-reduced-pom.xml
[INFO] 
[INFO] --- maven-source-plugin:2.2.1:jar-no-fork (create-source-jar) @ spark-core_2.10 ---
[INFO] Building jar: /home/hadoop/develop/spark/core/target/spark-core_2.10-1.3.0-sources.jar
[INFO] 
[INFO] >>> scala-maven-plugin:3.2.0:doc-jar (attach-scaladocs) @ spark-core_2.10 >>>
[INFO] 
[INFO] --- maven-enforcer-plugin:1.3.1:enforce (enforce-versions) @ spark-core_2.10 ---
[INFO] 
[INFO] --- scala-maven-plugin:3.2.0:add-source (eclipse-add-source) @ spark-core_2.10 ---
[INFO] 
[INFO] --- build-helper-maven-plugin:1.8:add-source (add-scala-sources) @ spark-core_2.10 ---
[INFO] Source directory: /home/hadoop/develop/spark/core/src/main/scala added.
[INFO] 
[INFO] <<< scala-maven-plugin:3.2.0:doc-jar (attach-scaladocs) @ spark-core_2.10 <<<
[INFO] 
[INFO] --- scala-maven-plugin:3.2.0:doc-jar (attach-scaladocs) @ spark-core_2.10 ---
/home/hadoop/develop/spark/core/src/main/scala/org/apache/spark/scheduler/local/LocalBackend.scala:89: warning: postfix operator millis should be enabled
by making the implicit value scala.language.postfixOps visible.
This can be achieved by adding the import clause 'import scala.language.postfixOps'
or by setting the compiler option -language:postfixOps.
See the Scala docs for value scala.language.postfixOps for a discussion
why the feature should be explicitly enabled.
      context.system.scheduler.scheduleOnce(1000 millis, self, ReviveOffers)
                                                 ^
/home/hadoop/develop/spark/core/src/main/scala/org/apache/spark/util/MutableURLClassLoader.scala:26: warning: imported `ParentClassLoader' is permanently hidden by definition of class ParentClassLoader in package util
import org.apache.spark.util.ParentClassLoader
                             ^
model contains 290 documentable templates
/home/hadoop/develop/spark/core/src/main/scala/org/apache/spark/TaskEndReason.scala:51: warning: Could not find any member to link for "org.apache.spark.scheduler.ShuffleMapTask".
/**
^
/home/hadoop/develop/spark/core/src/main/scala/org/apache/spark/scheduler/package.scala:20: warning: Could not find any member to link for "org.apache.spark.scheduler.DAGScheduler".
/**
^
/home/hadoop/develop/spark/core/src/main/scala/org/apache/spark/rdd/RDD.scala:575: warning: The link target "PairRDDFunctions.aggregateByKey" is ambiguous. Several members fit the target:
[U](zeroValue: U)(seqOp: (U, V) => U,combOp: (U, U) => U)(implicit evidence$3: scala.reflect.ClassTag[U]): org.apache.spark.rdd.RDD[(K, U)] in class PairRDDFunctions [chosen]
[U](zeroValue: U,numPartitions: Int)(seqOp: (U, V) => U,combOp: (U, U) => U)(implicit evidence$2: scala.reflect.ClassTag[U]): org.apache.spark.rdd.RDD[(K, U)] in class PairRDDFunctions
[U](zeroValue: U,partitioner: org.apache.spark.Partitioner)(seqOp: (U, V) => U,combOp: (U, U) => U)(implicit evidence$1: scala.reflect.ClassTag[U]): org.apache.spark.rdd.RDD[(K, U)] in class PairRDDFunctions


Quick crash course on using Scaladoc links
==========================================
Disambiguating terms and types: Prefix terms with '$' and types with '!' in case both names are in use:
 - [[scala.collection.immutable.List!.apply class List's apply method]] and
 - [[scala.collection.immutable.List$.apply object List's apply method]]
Disambiguating overloaded members: If a term is overloaded, you can indicate the first part of its signature followed by *:
 - [[[scala.collection.immutable.List$.fill[A](Int)(â‡’A):List[A]* Fill with a single parameter]]]
 - [[[scala.collection.immutable.List$.fill[A](Int,Int)(â‡’A):List[List[A]]* Fill with a two parameters]]]
Notes:
 - you can use any number of matching square brackets to avoid interference with the signature
 - you can use \\. to escape dots in prefixes (don't forget to use * at the end to match the signature!)
 - you can use \\# to escape hashes, otherwise they will be considered as delimiters, like dots.
  /**
  ^
/home/hadoop/develop/spark/core/src/main/scala/org/apache/spark/rdd/RDD.scala:563: warning: The link target "PairRDDFunctions.aggregateByKey" is ambiguous. Several members fit the target:
[U](zeroValue: U)(seqOp: (U, V) => U,combOp: (U, U) => U)(implicit evidence$3: scala.reflect.ClassTag[U]): org.apache.spark.rdd.RDD[(K, U)] in class PairRDDFunctions [chosen]
[U](zeroValue: U,numPartitions: Int)(seqOp: (U, V) => U,combOp: (U, U) => U)(implicit evidence$2: scala.reflect.ClassTag[U]): org.apache.spark.rdd.RDD[(K, U)] in class PairRDDFunctions
[U](zeroValue: U,partitioner: org.apache.spark.Partitioner)(seqOp: (U, V) => U,combOp: (U, U) => U)(implicit evidence$1: scala.reflect.ClassTag[U]): org.apache.spark.rdd.RDD[(K, U)] in class PairRDDFunctions


  /**
  ^
/home/hadoop/develop/spark/core/src/main/scala/org/apache/spark/rdd/RDD.scala:551: warning: The link target "PairRDDFunctions.aggregateByKey" is ambiguous. Several members fit the target:
[U](zeroValue: U)(seqOp: (U, V) => U,combOp: (U, U) => U)(implicit evidence$3: scala.reflect.ClassTag[U]): org.apache.spark.rdd.RDD[(K, U)] in class PairRDDFunctions [chosen]
[U](zeroValue: U,numPartitions: Int)(seqOp: (U, V) => U,combOp: (U, U) => U)(implicit evidence$2: scala.reflect.ClassTag[U]): org.apache.spark.rdd.RDD[(K, U)] in class PairRDDFunctions
[U](zeroValue: U,partitioner: org.apache.spark.Partitioner)(seqOp: (U, V) => U,combOp: (U, U) => U)(implicit evidence$1: scala.reflect.ClassTag[U]): org.apache.spark.rdd.RDD[(K, U)] in class PairRDDFunctions


  /**
  ^
/home/hadoop/develop/spark/core/src/main/scala/org/apache/spark/rdd/RDD.scala:1530: warning: The link target "PairRDDFunctions.reduceByKey" is ambiguous. Several members fit the target:
(func: (V, V) => V): org.apache.spark.rdd.RDD[(K, V)] in class PairRDDFunctions [chosen]
(func: (V, V) => V,numPartitions: Int): org.apache.spark.rdd.RDD[(K, V)] in class PairRDDFunctions
(partitioner: org.apache.spark.Partitioner,func: (V, V) => V): org.apache.spark.rdd.RDD[(K, V)] in class PairRDDFunctions


/**
^
/home/hadoop/develop/spark/core/src/main/scala/org/apache/spark/rdd/PairRDDFunctions.scala:556: warning: The link target "PairRDDFunctions.aggregateByKey" is ambiguous. Several members fit the target:
[U](zeroValue: U)(seqOp: (U, V) => U,combOp: (U, U) => U)(implicit evidence$3: scala.reflect.ClassTag[U]): org.apache.spark.rdd.RDD[(K, U)] in class PairRDDFunctions [chosen]
[U](zeroValue: U,numPartitions: Int)(seqOp: (U, V) => U,combOp: (U, U) => U)(implicit evidence$2: scala.reflect.ClassTag[U]): org.apache.spark.rdd.RDD[(K, U)] in class PairRDDFunctions
[U](zeroValue: U,partitioner: org.apache.spark.Partitioner)(seqOp: (U, V) => U,combOp: (U, U) => U)(implicit evidence$1: scala.reflect.ClassTag[U]): org.apache.spark.rdd.RDD[(K, U)] in class PairRDDFunctions


  /**
  ^
/home/hadoop/develop/spark/core/src/main/scala/org/apache/spark/rdd/PairRDDFunctions.scala:456: warning: The link target "PairRDDFunctions.aggregateByKey" is ambiguous. Several members fit the target:
[U](zeroValue: U)(seqOp: (U, V) => U,combOp: (U, U) => U)(implicit evidence$3: scala.reflect.ClassTag[U]): org.apache.spark.rdd.RDD[(K, U)] in class PairRDDFunctions [chosen]
[U](zeroValue: U,numPartitions: Int)(seqOp: (U, V) => U,combOp: (U, U) => U)(implicit evidence$2: scala.reflect.ClassTag[U]): org.apache.spark.rdd.RDD[(K, U)] in class PairRDDFunctions
[U](zeroValue: U,partitioner: org.apache.spark.Partitioner)(seqOp: (U, V) => U,combOp: (U, U) => U)(implicit evidence$1: scala.reflect.ClassTag[U]): org.apache.spark.rdd.RDD[(K, U)] in class PairRDDFunctions


  /**
  ^
/home/hadoop/develop/spark/core/src/main/scala/org/apache/spark/rdd/PairRDDFunctions.scala:431: warning: The link target "PairRDDFunctions.aggregateByKey" is ambiguous. Several members fit the target:
[U](zeroValue: U)(seqOp: (U, V) => U,combOp: (U, U) => U)(implicit evidence$3: scala.reflect.ClassTag[U]): org.apache.spark.rdd.RDD[(K, U)] in class PairRDDFunctions [chosen]
[U](zeroValue: U,numPartitions: Int)(seqOp: (U, V) => U,combOp: (U, U) => U)(implicit evidence$2: scala.reflect.ClassTag[U]): org.apache.spark.rdd.RDD[(K, U)] in class PairRDDFunctions
[U](zeroValue: U,partitioner: org.apache.spark.Partitioner)(seqOp: (U, V) => U,combOp: (U, U) => U)(implicit evidence$1: scala.reflect.ClassTag[U]): org.apache.spark.rdd.RDD[(K, U)] in class PairRDDFunctions


  /**
  ^
/home/hadoop/develop/spark/core/src/main/scala/org/apache/spark/rdd/NewHadoopRDD.scala:51: warning: Could not find any member to link for "org.apache.spark.SparkContext.newAPIHadoopRDD()".
/**
^
/home/hadoop/develop/spark/core/src/main/scala/org/apache/spark/rdd/HadoopRDD.scala:81: warning: Could not find any member to link for "org.apache.spark.SparkContext.hadoopRDD()".
/**
^
/home/hadoop/develop/spark/core/src/main/scala/org/apache/spark/executor/package.scala:20: warning: Could not find any member to link for "org.apache.spark.executor.Executor".
/**
^
/home/hadoop/develop/spark/core/src/main/scala/org/apache/spark/deploy/history/HistoryServer.scala:170: warning: Variable SPARK undefined in comment for object HistoryServer in object HistoryServer
object HistoryServer extends Logging {
       ^
/home/hadoop/develop/spark/core/src/main/scala/org/apache/spark/deploy/SparkHadoopUtil.scala:190: warning: Could not find any member to link for "FileStatus".
  /**
  ^
/home/hadoop/develop/spark/core/src/main/scala/org/apache/spark/broadcast/TorrentBroadcastFactory.scala:24: warning: Could not find any member to link for "org.apache.spark.broadcast.TorrentBroadcast".
/**
^
/home/hadoop/develop/spark/core/src/main/scala/org/apache/spark/broadcast/HttpBroadcastFactory.scala:24: warning: Could not find any member to link for "org.apache.spark.broadcast.HttpBroadcast".
/**
^
/home/hadoop/develop/spark/core/src/main/scala/org/apache/spark/Accumulators.scala:201: warning: The link target "SparkContext#accumulator" is ambiguous. Several members fit the target:
[T](initialValue: T,name: String)(implicit param: org.apache.spark.AccumulatorParam[T]): org.apache.spark.Accumulator[T] in class SparkContext [chosen]
[T](initialValue: T)(implicit param: org.apache.spark.AccumulatorParam[T]): org.apache.spark.Accumulator[T] in class SparkContext


/**
^
/home/hadoop/develop/spark/core/src/main/scala/org/apache/spark/api/java/JavaSparkContext.scala:797: warning: The link target "org.apache.spark.api.java.JavaSparkContext.setJobGroup" is ambiguous. Several members fit the target:
(groupId: String,description: String): Unit in class JavaSparkContext [chosen]
(groupId: String,description: String,interruptOnCancel: Boolean): Unit in class JavaSparkContext


  /**
  ^
/home/hadoop/develop/spark/core/src/main/scala/org/apache/spark/api/java/JavaRDDLike.scala:399: warning: The link target "org.apache.spark.api.java.JavaRDDLike#treeAggregate" is ambiguous. Several members fit the target:
[U](zeroValue: U,seqOp: org.apache.spark.api.java.function.Function2[U,T,U],combOp: org.apache.spark.api.java.function.Function2[U,U,U]): U in trait JavaRDDLike [chosen]
[U](zeroValue: U,seqOp: org.apache.spark.api.java.function.Function2[U,T,U],combOp: org.apache.spark.api.java.function.Function2[U,U,U],depth: Int): U in trait JavaRDDLike


  /**
  ^
/home/hadoop/develop/spark/core/src/main/scala/org/apache/spark/api/java/JavaRDDLike.scala:359: warning: The link target "org.apache.spark.api.java.JavaRDDLike#treeReduce" is ambiguous. Several members fit the target:
(f: org.apache.spark.api.java.function.Function2[T,T,T]): T in trait JavaRDDLike [chosen]
(f: org.apache.spark.api.java.function.Function2[T,T,T],depth: Int): T in trait JavaRDDLike


  /**
  ^
/home/hadoop/develop/spark/core/src/main/scala/org/apache/spark/api/java/JavaPairRDD.scala:510: warning: The link target "JavaPairRDD.reduceByKey" is ambiguous. Several members fit the target:
(func: org.apache.spark.api.java.function.Function2[V,V,V]): org.apache.spark.api.java.JavaPairRDD[K,V] in class JavaPairRDD [chosen]
(func: org.apache.spark.api.java.function.Function2[V,V,V],numPartitions: Int): org.apache.spark.api.java.JavaPairRDD[K,V] in class JavaPairRDD
(partitioner: org.apache.spark.Partitioner,func: org.apache.spark.api.java.function.Function2[V,V,V]): org.apache.spark.api.java.JavaPairRDD[K,V] in class JavaPairRDD


  /**
  ^
/home/hadoop/develop/spark/core/src/main/scala/org/apache/spark/api/java/JavaPairRDD.scala:380: warning: The link target "JavaPairRDD.reduceByKey" is ambiguous. Several members fit the target:
(func: org.apache.spark.api.java.function.Function2[V,V,V]): org.apache.spark.api.java.JavaPairRDD[K,V] in class JavaPairRDD [chosen]
(func: org.apache.spark.api.java.function.Function2[V,V,V],numPartitions: Int): org.apache.spark.api.java.JavaPairRDD[K,V] in class JavaPairRDD
(partitioner: org.apache.spark.Partitioner,func: org.apache.spark.api.java.function.Function2[V,V,V]): org.apache.spark.api.java.JavaPairRDD[K,V] in class JavaPairRDD


  /**
  ^
/home/hadoop/develop/spark/core/src/main/scala/org/apache/spark/api/java/JavaPairRDD.scala:369: warning: The link target "JavaPairRDD.reduceByKey" is ambiguous. Several members fit the target:
(func: org.apache.spark.api.java.function.Function2[V,V,V]): org.apache.spark.api.java.JavaPairRDD[K,V] in class JavaPairRDD [chosen]
(func: org.apache.spark.api.java.function.Function2[V,V,V],numPartitions: Int): org.apache.spark.api.java.JavaPairRDD[K,V] in class JavaPairRDD
(partitioner: org.apache.spark.Partitioner,func: org.apache.spark.api.java.function.Function2[V,V,V]): org.apache.spark.api.java.JavaPairRDD[K,V] in class JavaPairRDD


  /**
  ^
/home/hadoop/develop/spark/core/src/main/scala/org/apache/spark/api/java/JavaPairRDD.scala:177: warning: The link target "sampleByKey" is ambiguous. Several members fit the target:
(withReplacement: Boolean,fractions: java.util.Map[K,Double]): org.apache.spark.api.java.JavaPairRDD[K,V] in class JavaPairRDD [chosen]
(withReplacement: Boolean,fractions: java.util.Map[K,Double],seed: Long): org.apache.spark.api.java.JavaPairRDD[K,V] in class JavaPairRDD


  /**
  ^
/home/hadoop/develop/spark/core/src/main/scala/org/apache/spark/api/java/JavaPairRDD.scala:160: warning: The link target "sampleByKey" is ambiguous. Several members fit the target:
(withReplacement: Boolean,fractions: java.util.Map[K,Double]): org.apache.spark.api.java.JavaPairRDD[K,V] in class JavaPairRDD [chosen]
(withReplacement: Boolean,fractions: java.util.Map[K,Double],seed: Long): org.apache.spark.api.java.JavaPairRDD[K,V] in class JavaPairRDD


  /**
  ^
/home/hadoop/develop/spark/core/src/main/scala/org/apache/spark/network/nio/NioBlockTransferService.scala:32: warning: Could not find any member to link for "BlockTransferService".
/**
^
/home/hadoop/develop/spark/core/src/main/scala/org/apache/spark/network/BlockTransferService.scala:81: warning: The link target "init" is ambiguous. Several members fit the target:
(blockDataManager: org.apache.spark.network.BlockDataManager): Unit in class NioBlockTransferService [chosen]
(x$1: String): Unit in class NioBlockTransferService


  /**
  ^
/home/hadoop/develop/spark/core/src/main/scala/org/apache/spark/network/BlockTransferService.scala:55: warning: The link target "init" is ambiguous. Several members fit the target:
(blockDataManager: org.apache.spark.network.BlockDataManager): Unit in class NioBlockTransferService [chosen]
(x$1: String): Unit in class NioBlockTransferService


  /**
  ^
/home/hadoop/develop/spark/core/src/main/scala/org/apache/spark/network/nio/NioBlockTransferService.scala:51: warning: The link target "init" is ambiguous. Several members fit the target:
(blockDataManager: org.apache.spark.network.BlockDataManager): Unit in class NioBlockTransferService [chosen]
(x$1: String): Unit in class NioBlockTransferService


  /**
  ^
/home/hadoop/develop/spark/core/src/main/scala/org/apache/spark/network/nio/NioBlockTransferService.scala:43: warning: The link target "init" is ambiguous. Several members fit the target:
(blockDataManager: org.apache.spark.network.BlockDataManager): Unit in class NioBlockTransferService [chosen]
(x$1: String): Unit in class NioBlockTransferService


  /**
  ^
/home/hadoop/develop/spark/core/src/main/scala/org/apache/spark/network/nio/NioBlockTransferService.scala:132: warning: The link target "init" is ambiguous. Several members fit the target:
(blockDataManager: org.apache.spark.network.BlockDataManager): Unit in class NioBlockTransferService [chosen]
(x$1: String): Unit in class NioBlockTransferService


  /**
  ^
/home/hadoop/develop/spark/core/src/main/scala/org/apache/spark/network/BlockTransferService.scala:105: warning: The link target "init" is ambiguous. Several members fit the target:
(blockDataManager: org.apache.spark.network.BlockDataManager): Unit in class NioBlockTransferService [chosen]
(x$1: String): Unit in class NioBlockTransferService


  /**
  ^
/home/hadoop/develop/spark/core/src/main/scala/org/apache/spark/network/netty/SparkTransportConf.scala:41: warning: Could not find any member to link for "TransportConf".
  /**
  ^
/home/hadoop/develop/spark/core/src/main/scala/org/apache/spark/network/BlockTransferService.scala:50: warning: The link target "init" is ambiguous. Several members fit the target:
(blockDataManager: org.apache.spark.network.BlockDataManager): Unit in class NettyBlockTransferService [chosen]
(x$1: String): Unit in class NettyBlockTransferService


  /**
  ^
/home/hadoop/develop/spark/core/src/main/scala/org/apache/spark/network/BlockTransferService.scala:45: warning: The link target "init" is ambiguous. Several members fit the target:
(blockDataManager: org.apache.spark.network.BlockDataManager): Unit in class NettyBlockTransferService [chosen]
(x$1: String): Unit in class NettyBlockTransferService


  /**
  ^
/home/hadoop/develop/spark/core/src/main/scala/org/apache/spark/network/BlockTransferService.scala:70: warning: The link target "init" is ambiguous. Several members fit the target:
(blockDataManager: org.apache.spark.network.BlockDataManager): Unit in class NettyBlockTransferService [chosen]
(x$1: String): Unit in class NettyBlockTransferService


  /**
  ^
38 warnings found
[INFO] Building jar: /home/hadoop/develop/spark/core/target/spark-core_2.10-1.3.0-javadoc.jar
[INFO]  already added, skipping
[INFO] 
[INFO] --- maven-install-plugin:2.5.1:install (default-install) @ spark-core_2.10 ---
[INFO] Installing /home/hadoop/develop/spark/core/target/spark-core_2.10-1.3.0.jar to /home/hadoop/develop/m2repo/org/apache/spark/spark-core_2.10/1.3.0/spark-core_2.10-1.3.0.jar
[INFO] Installing /home/hadoop/develop/spark/core/dependency-reduced-pom.xml to /home/hadoop/develop/m2repo/org/apache/spark/spark-core_2.10/1.3.0/spark-core_2.10-1.3.0.pom
[INFO] Installing /home/hadoop/develop/spark/core/target/spark-core_2.10-1.3.0-sources.jar to /home/hadoop/develop/m2repo/org/apache/spark/spark-core_2.10/1.3.0/spark-core_2.10-1.3.0-sources.jar
[INFO] Installing /home/hadoop/develop/spark/core/target/spark-core_2.10-1.3.0-javadoc.jar to /home/hadoop/develop/m2repo/org/apache/spark/spark-core_2.10/1.3.0/spark-core_2.10-1.3.0-javadoc.jar
[INFO] ------------------------------------------------------------------------
[INFO] BUILD SUCCESS
[INFO] ------------------------------------------------------------------------
[INFO] Total time: 7:23.508s
[INFO] Finished at: Sun May 22 22:58:40 CST 2016
[INFO] Final Memory: 47M/723M
[INFO] ------------------------------------------------------------------------
