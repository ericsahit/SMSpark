[INFO] Scanning for projects...
[INFO]                                                                         
[INFO] ------------------------------------------------------------------------
[INFO] Building Spark Project Core 1.3.0
[INFO] ------------------------------------------------------------------------
[INFO] 
[INFO] --- maven-enforcer-plugin:1.3.1:enforce (enforce-versions) @ spark-core_2.10 ---
[INFO] 
[INFO] --- scala-maven-plugin:3.2.0:add-source (eclipse-add-source) @ spark-core_2.10 ---
[INFO] Add Source directory: /home/hadoop/develop/spark/core/src/main/scala
[INFO] Add Test Source directory: /home/hadoop/develop/spark/core/src/test/scala
[INFO] 
[INFO] --- build-helper-maven-plugin:1.8:add-source (add-scala-sources) @ spark-core_2.10 ---
[INFO] Source directory: /home/hadoop/develop/spark/core/src/main/scala added.
[INFO] 
[INFO] --- maven-remote-resources-plugin:1.5:process (default) @ spark-core_2.10 ---
[INFO] 
[INFO] --- maven-antrun-plugin:1.7:run (default) @ spark-core_2.10 ---
[INFO] Executing tasks

main:
    [unzip] Expanding: /home/hadoop/develop/spark/python/lib/py4j-0.8.2.1-src.zip into /home/hadoop/develop/spark/python/build
[INFO] Executed tasks
[INFO] 
[INFO] --- maven-resources-plugin:2.6:resources (default-resources) @ spark-core_2.10 ---
[INFO] Using 'UTF-8' encoding to copy filtered resources.
[INFO] Copying 11 resources
[INFO] Copying 24 resources
[INFO] Copying 7 resources
[INFO] Copying 3 resources
[INFO] 
[INFO] --- scala-maven-plugin:3.2.0:compile (scala-compile-first) @ spark-core_2.10 ---
[INFO] Using zinc server for incremental compilation
[INFO] compiler plugin: BasicArtifact(org.scalamacros,paradise_2.10.4,2.0.1,null)
[0m[[0minfo[0m] [0mCompiling 4 Scala sources and 32 Java sources to /home/hadoop/develop/spark/core/target/scala-2.10/classes...[0m
[0m[[33mwarn[0m] [0mwarning: [options] bootstrap class path not set in conjunction with -source 1.6[0m
[0m[[33mwarn[0m] [0m1 warning[0m
[0m[[0minfo[0m] [0mCompiling 42 Scala sources to /home/hadoop/develop/spark/core/target/scala-2.10/classes...[0m
[0m[[0minfo[0m] [0mCompile success at Aug 11, 2015 10:10:43 AM [2:00.632s][0m
[INFO] 
[INFO] --- maven-compiler-plugin:3.1:compile (default-compile) @ spark-core_2.10 ---
[INFO] Changes detected - recompiling the module!
[INFO] Compiling 44 source files to /home/hadoop/develop/spark/core/target/scala-2.10/classes
[INFO] 
[INFO] --- build-helper-maven-plugin:1.8:add-test-source (add-scala-test-sources) @ spark-core_2.10 ---
[INFO] Test Source directory: /home/hadoop/develop/spark/core/src/test/scala added.
[INFO] 
[INFO] --- maven-resources-plugin:2.6:testResources (default-testResources) @ spark-core_2.10 ---
[INFO] Using 'UTF-8' encoding to copy filtered resources.
[INFO] Copying 7 resources
[INFO] Copying 3 resources
[INFO] 
[INFO] --- scala-maven-plugin:3.2.0:testCompile (scala-test-compile-first) @ spark-core_2.10 ---
[INFO] Using zinc server for incremental compilation
[INFO] compiler plugin: BasicArtifact(org.scalamacros,paradise_2.10.4,2.0.1,null)
[0m[[0minfo[0m] [0mCompiling 22 Scala sources to /home/hadoop/develop/spark/core/target/scala-2.10/test-classes...[0m
[0m[[0minfo[0m] [0mCompile success at Aug 11, 2015 10:12:06 AM [1:12.014s][0m
[INFO] 
[INFO] --- maven-compiler-plugin:3.1:testCompile (default-testCompile) @ spark-core_2.10 ---
[INFO] Nothing to compile - all classes are up to date
[INFO] 
[INFO] --- maven-dependency-plugin:2.9:build-classpath (default) @ spark-core_2.10 ---
[INFO] Skipped writing classpath file '/home/hadoop/develop/spark/core/target/spark-test-classpath.txt'.  No changes found.
[INFO] 
[INFO] --- gmavenplus-plugin:1.2:execute (default) @ spark-core_2.10 ---
[INFO] Using Groovy 2.3.7 to perform execute.
[INFO] 
[INFO] --- maven-surefire-plugin:2.18:test (default-test) @ spark-core_2.10 ---
[INFO] Tests are skipped.
[INFO] 
[INFO] --- scalatest-maven-plugin:1.0:test (test) @ spark-core_2.10 ---
[INFO] Tests are skipped.
[INFO] 
[INFO] --- maven-jar-plugin:2.4:jar (default-jar) @ spark-core_2.10 ---
[INFO] Building jar: /home/hadoop/develop/spark/core/target/spark-core_2.10-1.3.0.jar
[INFO] 
[INFO] --- maven-site-plugin:3.3:attach-descriptor (attach-descriptor) @ spark-core_2.10 ---
[INFO] 
[INFO] --- maven-dependency-plugin:2.9:copy-dependencies (copy-dependencies) @ spark-core_2.10 ---
[INFO] 
[INFO] --- maven-shade-plugin:2.2:shade (default) @ spark-core_2.10 ---
[INFO] Excluding com.twitter:chill_2.10:jar:0.5.0 from the shaded jar.
[INFO] Excluding com.esotericsoftware.kryo:kryo:jar:2.21 from the shaded jar.
[INFO] Excluding com.esotericsoftware.reflectasm:reflectasm:jar:shaded:1.07 from the shaded jar.
[INFO] Excluding com.esotericsoftware.minlog:minlog:jar:1.2 from the shaded jar.
[INFO] Excluding org.objenesis:objenesis:jar:1.2 from the shaded jar.
[INFO] Excluding com.twitter:chill-java:jar:0.5.0 from the shaded jar.
[INFO] Excluding org.apache.hadoop:hadoop-client:jar:2.3.0 from the shaded jar.
[INFO] Excluding org.apache.hadoop:hadoop-common:jar:2.3.0 from the shaded jar.
[INFO] Excluding commons-cli:commons-cli:jar:1.2 from the shaded jar.
[INFO] Excluding xmlenc:xmlenc:jar:0.52 from the shaded jar.
[INFO] Excluding commons-httpclient:commons-httpclient:jar:3.1 from the shaded jar.
[INFO] Excluding commons-io:commons-io:jar:2.4 from the shaded jar.
[INFO] Excluding commons-collections:commons-collections:jar:3.2.1 from the shaded jar.
[INFO] Excluding commons-lang:commons-lang:jar:2.6 from the shaded jar.
[INFO] Excluding commons-configuration:commons-configuration:jar:1.6 from the shaded jar.
[INFO] Excluding commons-digester:commons-digester:jar:1.8 from the shaded jar.
[INFO] Excluding commons-beanutils:commons-beanutils:jar:1.7.0 from the shaded jar.
[INFO] Excluding commons-beanutils:commons-beanutils-core:jar:1.8.0 from the shaded jar.
[INFO] Excluding org.apache.avro:avro:jar:1.7.6 from the shaded jar.
[INFO] Excluding com.google.protobuf:protobuf-java:jar:2.5.0 from the shaded jar.
[INFO] Excluding org.apache.hadoop:hadoop-auth:jar:2.3.0 from the shaded jar.
[INFO] Excluding org.apache.commons:commons-compress:jar:1.4.1 from the shaded jar.
[INFO] Excluding org.tukaani:xz:jar:1.0 from the shaded jar.
[INFO] Excluding org.apache.hadoop:hadoop-hdfs:jar:2.3.0 from the shaded jar.
[INFO] Excluding org.mortbay.jetty:jetty-util:jar:6.1.26 from the shaded jar.
[INFO] Excluding org.apache.hadoop:hadoop-mapreduce-client-app:jar:2.3.0 from the shaded jar.
[INFO] Excluding org.apache.hadoop:hadoop-mapreduce-client-common:jar:2.3.0 from the shaded jar.
[INFO] Excluding org.apache.hadoop:hadoop-yarn-client:jar:2.3.0 from the shaded jar.
[INFO] Excluding org.apache.hadoop:hadoop-yarn-server-common:jar:2.3.0 from the shaded jar.
[INFO] Excluding org.apache.hadoop:hadoop-mapreduce-client-shuffle:jar:2.3.0 from the shaded jar.
[INFO] Excluding org.apache.hadoop:hadoop-yarn-api:jar:2.3.0 from the shaded jar.
[INFO] Excluding org.apache.hadoop:hadoop-mapreduce-client-core:jar:2.3.0 from the shaded jar.
[INFO] Excluding org.apache.hadoop:hadoop-yarn-common:jar:2.3.0 from the shaded jar.
[INFO] Excluding javax.xml.bind:jaxb-api:jar:2.2.2 from the shaded jar.
[INFO] Excluding javax.xml.stream:stax-api:jar:1.0-2 from the shaded jar.
[INFO] Excluding com.sun.jersey:jersey-core:jar:1.9 from the shaded jar.
[INFO] Excluding org.apache.hadoop:hadoop-mapreduce-client-jobclient:jar:2.3.0 from the shaded jar.
[INFO] Excluding org.apache.hadoop:hadoop-annotations:jar:2.3.0 from the shaded jar.
[INFO] Excluding org.apache.spark:spark-network-common_2.10:jar:1.3.0 from the shaded jar.
[INFO] Excluding org.apache.spark:spark-network-shuffle_2.10:jar:1.3.0 from the shaded jar.
[INFO] Excluding net.java.dev.jets3t:jets3t:jar:0.9.3 from the shaded jar.
[INFO] Excluding org.apache.httpcomponents:httpcore:jar:4.3.3 from the shaded jar.
[INFO] Excluding org.apache.httpcomponents:httpclient:jar:4.3.6 from the shaded jar.
[INFO] Excluding commons-codec:commons-codec:jar:1.10 from the shaded jar.
[INFO] Excluding org.codehaus.jackson:jackson-core-asl:jar:1.9.13 from the shaded jar.
[INFO] Excluding org.codehaus.jackson:jackson-mapper-asl:jar:1.9.13 from the shaded jar.
[INFO] Excluding javax.activation:activation:jar:1.1.1 from the shaded jar.
[INFO] Excluding mx4j:mx4j:jar:3.0.2 from the shaded jar.
[INFO] Excluding javax.mail:mail:jar:1.4.7 from the shaded jar.
[INFO] Excluding org.bouncycastle:bcprov-jdk15on:jar:1.51 from the shaded jar.
[INFO] Excluding com.jamesmurty.utils:java-xmlbuilder:jar:1.0 from the shaded jar.
[INFO] Excluding net.iharder:base64:jar:2.3.8 from the shaded jar.
[INFO] Excluding org.apache.curator:curator-recipes:jar:2.4.0 from the shaded jar.
[INFO] Excluding org.apache.curator:curator-framework:jar:2.4.0 from the shaded jar.
[INFO] Excluding org.apache.curator:curator-client:jar:2.4.0 from the shaded jar.
[INFO] Excluding org.apache.zookeeper:zookeeper:jar:3.4.5 from the shaded jar.
[INFO] Excluding jline:jline:jar:0.9.94 from the shaded jar.
[INFO] Including org.eclipse.jetty:jetty-plus:jar:8.1.14.v20131031 in the shaded jar.
[INFO] Excluding org.eclipse.jetty.orbit:javax.transaction:jar:1.1.1.v201105210645 from the shaded jar.
[INFO] Excluding org.eclipse.jetty:jetty-webapp:jar:8.1.14.v20131031 from the shaded jar.
[INFO] Excluding org.eclipse.jetty:jetty-xml:jar:8.1.14.v20131031 from the shaded jar.
[INFO] Excluding org.eclipse.jetty:jetty-jndi:jar:8.1.14.v20131031 from the shaded jar.
[INFO] Excluding org.eclipse.jetty.orbit:javax.mail.glassfish:jar:1.4.1.v201005082020 from the shaded jar.
[INFO] Excluding org.eclipse.jetty.orbit:javax.activation:jar:1.1.0.v201105071233 from the shaded jar.
[INFO] Including org.eclipse.jetty:jetty-security:jar:8.1.14.v20131031 in the shaded jar.
[INFO] Including org.eclipse.jetty:jetty-util:jar:8.1.14.v20131031 in the shaded jar.
[INFO] Including org.eclipse.jetty:jetty-server:jar:8.1.14.v20131031 in the shaded jar.
[INFO] Including org.eclipse.jetty:jetty-http:jar:8.1.14.v20131031 in the shaded jar.
[INFO] Including org.eclipse.jetty:jetty-io:jar:8.1.14.v20131031 in the shaded jar.
[INFO] Including org.eclipse.jetty:jetty-continuation:jar:8.1.14.v20131031 in the shaded jar.
[INFO] Including org.eclipse.jetty:jetty-servlet:jar:8.1.14.v20131031 in the shaded jar.
[INFO] Excluding org.eclipse.jetty.orbit:javax.servlet:jar:3.0.0.v201112011016 from the shaded jar.
[INFO] Excluding org.apache.commons:commons-lang3:jar:3.3.2 from the shaded jar.
[INFO] Excluding org.apache.commons:commons-math3:jar:3.1.1 from the shaded jar.
[INFO] Excluding com.google.code.findbugs:jsr305:jar:1.3.9 from the shaded jar.
[INFO] Excluding org.slf4j:slf4j-api:jar:1.7.10 from the shaded jar.
[INFO] Excluding org.slf4j:jul-to-slf4j:jar:1.7.10 from the shaded jar.
[INFO] Excluding org.slf4j:jcl-over-slf4j:jar:1.7.10 from the shaded jar.
[INFO] Excluding log4j:log4j:jar:1.2.17 from the shaded jar.
[INFO] Excluding org.slf4j:slf4j-log4j12:jar:1.7.10 from the shaded jar.
[INFO] Excluding com.ning:compress-lzf:jar:1.0.0 from the shaded jar.
[INFO] Excluding org.xerial.snappy:snappy-java:jar:1.1.1.6 from the shaded jar.
[INFO] Excluding net.jpountz.lz4:lz4:jar:1.2.0 from the shaded jar.
[INFO] Excluding org.roaringbitmap:RoaringBitmap:jar:0.4.5 from the shaded jar.
[INFO] Excluding commons-net:commons-net:jar:2.2 from the shaded jar.
[INFO] Excluding org.spark-project.akka:akka-remote_2.10:jar:2.3.4-spark from the shaded jar.
[INFO] Excluding org.spark-project.akka:akka-actor_2.10:jar:2.3.4-spark from the shaded jar.
[INFO] Excluding com.typesafe:config:jar:1.2.1 from the shaded jar.
[INFO] Excluding io.netty:netty:jar:3.8.0.Final from the shaded jar.
[INFO] Excluding org.spark-project.protobuf:protobuf-java:jar:2.5.0-spark from the shaded jar.
[INFO] Excluding org.uncommons.maths:uncommons-maths:jar:1.2.2a from the shaded jar.
[INFO] Excluding org.spark-project.akka:akka-slf4j_2.10:jar:2.3.4-spark from the shaded jar.
[INFO] Excluding org.scala-lang:scala-library:jar:2.10.4 from the shaded jar.
[INFO] Excluding org.json4s:json4s-jackson_2.10:jar:3.2.10 from the shaded jar.
[INFO] Excluding org.json4s:json4s-core_2.10:jar:3.2.10 from the shaded jar.
[INFO] Excluding org.json4s:json4s-ast_2.10:jar:3.2.10 from the shaded jar.
[INFO] Excluding org.scala-lang:scalap:jar:2.10.4 from the shaded jar.
[INFO] Excluding org.scala-lang:scala-compiler:jar:2.10.4 from the shaded jar.
[INFO] Excluding org.apache.mesos:mesos:jar:shaded-protobuf:0.21.0 from the shaded jar.
[INFO] Excluding io.netty:netty-all:jar:4.0.23.Final from the shaded jar.
[INFO] Excluding com.clearspring.analytics:stream:jar:2.7.0 from the shaded jar.
[INFO] Excluding io.dropwizard.metrics:metrics-core:jar:3.1.0 from the shaded jar.
[INFO] Excluding io.dropwizard.metrics:metrics-jvm:jar:3.1.0 from the shaded jar.
[INFO] Excluding io.dropwizard.metrics:metrics-json:jar:3.1.0 from the shaded jar.
[INFO] Excluding io.dropwizard.metrics:metrics-graphite:jar:3.1.0 from the shaded jar.
[INFO] Excluding com.fasterxml.jackson.core:jackson-databind:jar:2.4.4 from the shaded jar.
[INFO] Excluding com.fasterxml.jackson.core:jackson-annotations:jar:2.4.0 from the shaded jar.
[INFO] Excluding com.fasterxml.jackson.core:jackson-core:jar:2.4.4 from the shaded jar.
[INFO] Excluding com.fasterxml.jackson.module:jackson-module-scala_2.10:jar:2.4.4 from the shaded jar.
[INFO] Excluding org.scala-lang:scala-reflect:jar:2.10.4 from the shaded jar.
[INFO] Excluding com.thoughtworks.paranamer:paranamer:jar:2.6 from the shaded jar.
[INFO] Excluding org.apache.ivy:ivy:jar:2.4.0 from the shaded jar.
[INFO] Excluding oro:oro:jar:2.0.8 from the shaded jar.
[INFO] Excluding org.tachyonproject:tachyon-client:jar:0.5.0 from the shaded jar.
[INFO] Excluding org.tachyonproject:tachyon:jar:0.5.0 from the shaded jar.
[INFO] Excluding commons-logging:commons-logging:jar:1.1.3 from the shaded jar.
[INFO] Excluding org.spark-project:pyrolite:jar:2.0.1 from the shaded jar.
[INFO] Excluding net.sf.py4j:py4j:jar:0.8.2.1 from the shaded jar.
[INFO] Including org.spark-project.spark:unused:jar:1.0.0 in the shaded jar.
[INFO] Replacing original artifact with shaded artifact.
[INFO] Replacing /home/hadoop/develop/spark/core/target/spark-core_2.10-1.3.0.jar with /home/hadoop/develop/spark/core/target/spark-core_2.10-1.3.0-shaded.jar
[INFO] Dependency-reduced POM written at: /home/hadoop/develop/spark/core/dependency-reduced-pom.xml
[INFO] Dependency-reduced POM written at: /home/hadoop/develop/spark/core/dependency-reduced-pom.xml
[INFO] 
[INFO] --- maven-source-plugin:2.2.1:jar-no-fork (create-source-jar) @ spark-core_2.10 ---
[INFO] Building jar: /home/hadoop/develop/spark/core/target/spark-core_2.10-1.3.0-sources.jar
[INFO] 
[INFO] --- scalastyle-maven-plugin:0.4.0:check (default) @ spark-core_2.10 ---
error file=/home/hadoop/develop/spark/core/src/main/scala/org/apache/spark/deploy/worker/Worker.scala message=File line length exceeds 100 characters line=181
error file=/home/hadoop/develop/spark/core/src/main/scala/org/apache/spark/scheduler/DAGScheduler.scala message=File line length exceeds 100 characters line=208
error file=/home/hadoop/develop/spark/core/src/main/scala/org/apache/spark/CacheManager.scala message=File line length exceeds 100 characters line=51
error file=/home/hadoop/develop/spark/core/src/main/scala/org/apache/spark/storage/BlockManagerMasterActor.scala message=File line length exceeds 100 characters line=428
error file=/home/hadoop/develop/spark/core/src/main/scala/org/apache/spark/storage/BlockManager.scala message=File line length exceeds 100 characters line=191
error file=/home/hadoop/develop/spark/core/src/main/scala/org/apache/spark/storage/BlockManager.scala message=File line length exceeds 100 characters line=233
error file=/home/hadoop/develop/spark/core/src/main/scala/org/apache/spark/storage/BlockManager.scala message=File line length exceeds 100 characters line=480
error file=/home/hadoop/develop/spark/core/src/main/scala/org/apache/spark/storage/BlockManager.scala message=File line length exceeds 100 characters line=496
error file=/home/hadoop/develop/spark/core/src/main/scala/org/apache/spark/storage/BlockManager.scala message=File line length exceeds 100 characters line=497
error file=/home/hadoop/develop/spark/core/src/main/scala/org/apache/spark/storage/BlockManager.scala message=File line length exceeds 100 characters line=500
error file=/home/hadoop/develop/spark/core/src/main/scala/org/apache/spark/storage/BlockManager.scala message=File line length exceeds 100 characters line=701
error file=/home/hadoop/develop/spark/core/src/main/scala/org/apache/spark/storage/BlockManager.scala message=The number of parameters should not exceed 10 line=178 column=6
error file=/home/hadoop/develop/spark/core/src/main/scala/org/apache/spark/SparkEnv.scala message=File line length exceeds 100 characters line=312
error file=/home/hadoop/develop/spark/core/src/main/scala/org/apache/spark/smstorage/master/BlockWorkerInfo.scala message=Header does not match expected text line=1
error file=/home/hadoop/develop/spark/core/src/main/scala/org/apache/spark/smstorage/master/BlockServerMasterActor.scala message=Header does not match expected text line=1
error file=/home/hadoop/develop/spark/core/src/main/scala/org/apache/spark/smstorage/master/BlockServerMasterActor.scala message=File must end with newline character
error file=/home/hadoop/develop/spark/core/src/main/scala/org/apache/spark/smstorage/master/BlockServerMaster.scala message=Header does not match expected text line=1
error file=/home/hadoop/develop/spark/core/src/main/scala/org/apache/spark/smstorage/master/BlockServerMaster.scala message=File line length exceeds 100 characters line=79
error file=/home/hadoop/develop/spark/core/src/main/scala/org/apache/spark/smstorage/master/BlockServerMaster.scala message=If block needs braces line=221 column=4
error file=/home/hadoop/develop/spark/core/src/main/scala/org/apache/spark/smstorage/master/BlockServerMaster.scala message=File must end with newline character
error file=/home/hadoop/develop/spark/core/src/main/scala/org/apache/spark/smstorage/SBlockEntry.scala message=Header does not match expected text line=1
error file=/home/hadoop/develop/spark/core/src/main/scala/org/apache/spark/smstorage/SBlockEntry.scala message=If block needs braces line=75 column=4
error file=/home/hadoop/develop/spark/core/src/main/scala/org/apache/spark/smstorage/SBlockEntry.scala message=If block needs braces line=79 column=4
error file=/home/hadoop/develop/spark/core/src/main/scala/org/apache/spark/smstorage/SBlockEntry.scala message=File must end with newline character
error file=/home/hadoop/develop/spark/core/src/main/scala/org/apache/spark/smstorage/BlockServerClientId.scala message=Header does not match expected text line=1
error file=/home/hadoop/develop/spark/core/src/main/scala/org/apache/spark/smstorage/BlockServerClientId.scala message=File must end with newline character
error file=/home/hadoop/develop/spark/core/src/main/scala/org/apache/spark/smstorage/worker/BlockServerWorker.scala message=Header does not match expected text line=1
error file=/home/hadoop/develop/spark/core/src/main/scala/org/apache/spark/smstorage/worker/BlockServerWorker.scala message=File must end with newline character
error file=/home/hadoop/develop/spark/core/src/main/scala/org/apache/spark/smstorage/worker/SpaceManager.scala message=Header does not match expected text line=1
error file=/home/hadoop/develop/spark/core/src/main/scala/org/apache/spark/smstorage/worker/SpaceManager.scala message=File line length exceeds 100 characters line=40
error file=/home/hadoop/develop/spark/core/src/main/scala/org/apache/spark/smstorage/worker/SpaceManager.scala message=File must end with newline character
error file=/home/hadoop/develop/spark/core/src/main/scala/org/apache/spark/smstorage/worker/BlockIndexer.scala message=Header does not match expected text line=1
error file=/home/hadoop/develop/spark/core/src/main/scala/org/apache/spark/smstorage/worker/BlockIndexer.scala message=File must end with newline character
error file=/home/hadoop/develop/spark/core/src/main/scala/org/apache/spark/smstorage/worker/BlockServerWorkerActor.scala message=Header does not match expected text line=1
error file=/home/hadoop/develop/spark/core/src/main/scala/org/apache/spark/smstorage/worker/BlockServerWorkerActor.scala message=File line length exceeds 100 characters line=80
error file=/home/hadoop/develop/spark/core/src/main/scala/org/apache/spark/smstorage/worker/BlockServerWorkerActor.scala message=File line length exceeds 100 characters line=205
error file=/home/hadoop/develop/spark/core/src/main/scala/org/apache/spark/smstorage/worker/BlockServerWorkerActor.scala message=File line length exceeds 100 characters line=221
error file=/home/hadoop/develop/spark/core/src/main/scala/org/apache/spark/smstorage/worker/BlockServerWorkerActor.scala message=File line length exceeds 100 characters line=229
error file=/home/hadoop/develop/spark/core/src/main/scala/org/apache/spark/smstorage/worker/BlockServerWorkerActor.scala message=File line length exceeds 100 characters line=232
error file=/home/hadoop/develop/spark/core/src/main/scala/org/apache/spark/smstorage/worker/BlockServerWorkerActor.scala message=File line length exceeds 100 characters line=255
error file=/home/hadoop/develop/spark/core/src/main/scala/org/apache/spark/smstorage/worker/BlockServerWorkerActor.scala message=File line length exceeds 100 characters line=310
error file=/home/hadoop/develop/spark/core/src/main/scala/org/apache/spark/smstorage/worker/BlockServerWorkerActor.scala message=File line length exceeds 100 characters line=432
error file=/home/hadoop/develop/spark/core/src/main/scala/org/apache/spark/smstorage/worker/BlockServerWorkerActor.scala message=If block needs braces line=228 column=6
error file=/home/hadoop/develop/spark/core/src/main/scala/org/apache/spark/smstorage/worker/BlockServerWorkerActor.scala message=If block needs braces line=307 column=10
error file=/home/hadoop/develop/spark/core/src/main/scala/org/apache/spark/smstorage/worker/BlockServerWorkerActor.scala message=If block needs braces line=386 column=10
error file=/home/hadoop/develop/spark/core/src/main/scala/org/apache/spark/smstorage/worker/BlockServerWorkerActor.scala message=If block needs braces line=468 column=4
error file=/home/hadoop/develop/spark/core/src/main/scala/org/apache/spark/smstorage/worker/SMManager.scala message=Header does not match expected text line=1
error file=/home/hadoop/develop/spark/core/src/main/scala/org/apache/spark/smstorage/worker/SMManager.scala message=File must end with newline character
error file=/home/hadoop/develop/spark/core/src/main/scala/org/apache/spark/smstorage/worker/ProcfsBasedGetter.scala message=Header does not match expected text line=1
error file=/home/hadoop/develop/spark/core/src/main/scala/org/apache/spark/smstorage/worker/ProcfsBasedGetter.scala message=There should be a space before the plus (+) sign line=30 column=76
error file=/home/hadoop/develop/spark/core/src/main/scala/org/apache/spark/smstorage/worker/ProcfsBasedGetter.scala message=There should be a space before the plus (+) sign line=31 column=77
error file=/home/hadoop/develop/spark/core/src/main/scala/org/apache/spark/smstorage/worker/ProcfsBasedGetter.scala message=File line length exceeds 100 characters line=56
error file=/home/hadoop/develop/spark/core/src/main/scala/org/apache/spark/smstorage/worker/ExecutorWatcher.scala message=Header does not match expected text line=1
error file=/home/hadoop/develop/spark/core/src/main/scala/org/apache/spark/smstorage/worker/ExecutorWatcher.scala message=File line length exceeds 100 characters line=77
error file=/home/hadoop/develop/spark/core/src/main/scala/org/apache/spark/smstorage/worker/ExecutorWatcher.scala message=File line length exceeds 100 characters line=79
error file=/home/hadoop/develop/spark/core/src/main/scala/org/apache/spark/smstorage/worker/BlockServerClientInfo.scala message=Header does not match expected text line=1
error file=/home/hadoop/develop/spark/core/src/main/scala/org/apache/spark/smstorage/worker/BlockServerClientInfo.scala message=File must end with newline character
error file=/home/hadoop/develop/spark/core/src/main/scala/org/apache/spark/smstorage/BlockServerMessages.scala message=Header does not match expected text line=1
error file=/home/hadoop/develop/spark/core/src/main/scala/org/apache/spark/smstorage/BlockServerMessages.scala message=File line length exceeds 100 characters line=22
error file=/home/hadoop/develop/spark/core/src/main/scala/org/apache/spark/smstorage/BlockServerMessages.scala message=File line length exceeds 100 characters line=25
error file=/home/hadoop/develop/spark/core/src/main/scala/org/apache/spark/smstorage/BlockServerMessages.scala message=File line length exceeds 100 characters line=27
error file=/home/hadoop/develop/spark/core/src/main/scala/org/apache/spark/smstorage/BlockServerMessages.scala message=File line length exceeds 100 characters line=29
error file=/home/hadoop/develop/spark/core/src/main/scala/org/apache/spark/smstorage/BlockServerMessages.scala message=File line length exceeds 100 characters line=66
error file=/home/hadoop/develop/spark/core/src/main/scala/org/apache/spark/smstorage/BlockServerMessages.scala message=File line length exceeds 100 characters line=70
error file=/home/hadoop/develop/spark/core/src/main/scala/org/apache/spark/smstorage/BlockServerMessages.scala message=File line length exceeds 100 characters line=78
error file=/home/hadoop/develop/spark/core/src/main/scala/org/apache/spark/smstorage/BlockServerMessages.scala message=File line length exceeds 100 characters line=81
error file=/home/hadoop/develop/spark/core/src/main/scala/org/apache/spark/smstorage/BlockServerMessages.scala message=File line length exceeds 100 characters line=87
error file=/home/hadoop/develop/spark/core/src/main/scala/org/apache/spark/smstorage/BlockServerMessages.scala message=File line length exceeds 100 characters line=89
error file=/home/hadoop/develop/spark/core/src/main/scala/org/apache/spark/smstorage/BlockServerMessages.scala message=File must end with newline character
error file=/home/hadoop/develop/spark/core/src/main/scala/org/apache/spark/smstorage/client/LocalMemoryStore.scala message=Header does not match expected text line=1
error file=/home/hadoop/develop/spark/core/src/main/scala/org/apache/spark/smstorage/client/LocalMemoryStore.scala message=File line length exceeds 100 characters line=109
error file=/home/hadoop/develop/spark/core/src/main/scala/org/apache/spark/smstorage/client/LocalMemoryStore.scala message=File line length exceeds 100 characters line=180
error file=/home/hadoop/develop/spark/core/src/main/scala/org/apache/spark/smstorage/client/LocalMemoryStore.scala message=File line length exceeds 100 characters line=182
error file=/home/hadoop/develop/spark/core/src/main/scala/org/apache/spark/smstorage/client/LocalMemoryStore.scala message=File line length exceeds 100 characters line=201
error file=/home/hadoop/develop/spark/core/src/main/scala/org/apache/spark/smstorage/client/LocalMemoryStore.scala message=If block needs braces line=195 column=10
error file=/home/hadoop/develop/spark/core/src/main/scala/org/apache/spark/smstorage/client/LocalMemoryStore.scala message=File must end with newline character
error file=/home/hadoop/develop/spark/core/src/main/scala/org/apache/spark/smstorage/client/BlockServerClientActor.scala message=Header does not match expected text line=1
error file=/home/hadoop/develop/spark/core/src/main/scala/org/apache/spark/smstorage/client/BlockServerClientActor.scala message=File must end with newline character
error file=/home/hadoop/develop/spark/core/src/main/scala/org/apache/spark/smstorage/client/BlockServerWorkerRef.scala message=Header does not match expected text line=1
error file=/home/hadoop/develop/spark/core/src/main/scala/org/apache/spark/smstorage/client/BlockServerWorkerRef.scala message=File line length exceeds 100 characters line=24
error file=/home/hadoop/develop/spark/core/src/main/scala/org/apache/spark/smstorage/client/BlockServerWorkerRef.scala message=File line length exceeds 100 characters line=144
error file=/home/hadoop/develop/spark/core/src/main/scala/org/apache/spark/smstorage/client/BlockServerWorkerRef.scala message=File must end with newline character
error file=/home/hadoop/develop/spark/core/src/main/scala/org/apache/spark/smstorage/BlockServerWorkerId.scala message=Header does not match expected text line=1
error file=/home/hadoop/develop/spark/core/src/main/scala/org/apache/spark/smstorage/BlockServerWorkerId.scala message=File must end with newline character
error file=/home/hadoop/develop/spark/core/src/main/scala/org/apache/spark/smstorage/SBlockId.scala message=Header does not match expected text line=1
error file=/home/hadoop/develop/spark/core/src/main/scala/org/apache/spark/smstorage/SBlockId.scala message=File must end with newline character
Saving to outputFile=/home/hadoop/develop/spark/core/scalastyle-output.xml
Processed 434 file(s)
Found 86 errors
Found 0 warnings
Found 0 infos
Finished in 13085 ms
[WARNING] Scalastyle:check violations detected but failOnViolation set to false
[INFO] 
[INFO] >>> scala-maven-plugin:3.2.0:doc-jar (attach-scaladocs) @ spark-core_2.10 >>>
[INFO] 
[INFO] --- maven-enforcer-plugin:1.3.1:enforce (enforce-versions) @ spark-core_2.10 ---
[INFO] 
[INFO] --- scala-maven-plugin:3.2.0:add-source (eclipse-add-source) @ spark-core_2.10 ---
[INFO] 
[INFO] --- build-helper-maven-plugin:1.8:add-source (add-scala-sources) @ spark-core_2.10 ---
[INFO] Source directory: /home/hadoop/develop/spark/core/src/main/scala added.
[INFO] 
[INFO] <<< scala-maven-plugin:3.2.0:doc-jar (attach-scaladocs) @ spark-core_2.10 <<<
[INFO] 
[INFO] --- scala-maven-plugin:3.2.0:doc-jar (attach-scaladocs) @ spark-core_2.10 ---
/home/hadoop/develop/spark/core/src/main/scala/org/apache/spark/scheduler/local/LocalBackend.scala:89: warning: postfix operator millis should be enabled
by making the implicit value scala.language.postfixOps visible.
This can be achieved by adding the import clause 'import scala.language.postfixOps'
or by setting the compiler option -language:postfixOps.
See the Scala docs for value scala.language.postfixOps for a discussion
why the feature should be explicitly enabled.
      context.system.scheduler.scheduleOnce(1000 millis, self, ReviveOffers)
                                                 ^
/home/hadoop/develop/spark/core/src/main/scala/org/apache/spark/util/MutableURLClassLoader.scala:26: warning: imported `ParentClassLoader' is permanently hidden by definition of class ParentClassLoader in package util
import org.apache.spark.util.ParentClassLoader
                             ^
model contains 286 documentable templates
/home/hadoop/develop/spark/core/src/main/scala/org/apache/spark/TaskEndReason.scala:51: warning: Could not find any member to link for "org.apache.spark.scheduler.ShuffleMapTask".
/**
^
/home/hadoop/develop/spark/core/src/main/scala/org/apache/spark/scheduler/package.scala:20: warning: Could not find any member to link for "org.apache.spark.scheduler.DAGScheduler".
/**
^
/home/hadoop/develop/spark/core/src/main/scala/org/apache/spark/rdd/RDD.scala:570: warning: The link target "PairRDDFunctions.aggregateByKey" is ambiguous. Several members fit the target:
[U](zeroValue: U)(seqOp: (U, V) => U,combOp: (U, U) => U)(implicit evidence$3: scala.reflect.ClassTag[U]): org.apache.spark.rdd.RDD[(K, U)] in class PairRDDFunctions [chosen]
[U](zeroValue: U,numPartitions: Int)(seqOp: (U, V) => U,combOp: (U, U) => U)(implicit evidence$2: scala.reflect.ClassTag[U]): org.apache.spark.rdd.RDD[(K, U)] in class PairRDDFunctions
[U](zeroValue: U,partitioner: org.apache.spark.Partitioner)(seqOp: (U, V) => U,combOp: (U, U) => U)(implicit evidence$1: scala.reflect.ClassTag[U]): org.apache.spark.rdd.RDD[(K, U)] in class PairRDDFunctions


Quick crash course on using Scaladoc links
==========================================
Disambiguating terms and types: Prefix terms with '$' and types with '!' in case both names are in use:
 - [[scala.collection.immutable.List!.apply class List's apply method]] and
 - [[scala.collection.immutable.List$.apply object List's apply method]]
Disambiguating overloaded members: If a term is overloaded, you can indicate the first part of its signature followed by *:
 - [[[scala.collection.immutable.List$.fill[A](Int)(â‡’A):List[A]* Fill with a single parameter]]]
 - [[[scala.collection.immutable.List$.fill[A](Int,Int)(â‡’A):List[List[A]]* Fill with a two parameters]]]
Notes:
 - you can use any number of matching square brackets to avoid interference with the signature
 - you can use \\. to escape dots in prefixes (don't forget to use * at the end to match the signature!)
 - you can use \\# to escape hashes, otherwise they will be considered as delimiters, like dots.
  /**
  ^
/home/hadoop/develop/spark/core/src/main/scala/org/apache/spark/rdd/RDD.scala:558: warning: The link target "PairRDDFunctions.aggregateByKey" is ambiguous. Several members fit the target:
[U](zeroValue: U)(seqOp: (U, V) => U,combOp: (U, U) => U)(implicit evidence$3: scala.reflect.ClassTag[U]): org.apache.spark.rdd.RDD[(K, U)] in class PairRDDFunctions [chosen]
[U](zeroValue: U,numPartitions: Int)(seqOp: (U, V) => U,combOp: (U, U) => U)(implicit evidence$2: scala.reflect.ClassTag[U]): org.apache.spark.rdd.RDD[(K, U)] in class PairRDDFunctions
[U](zeroValue: U,partitioner: org.apache.spark.Partitioner)(seqOp: (U, V) => U,combOp: (U, U) => U)(implicit evidence$1: scala.reflect.ClassTag[U]): org.apache.spark.rdd.RDD[(K, U)] in class PairRDDFunctions


  /**
  ^
/home/hadoop/develop/spark/core/src/main/scala/org/apache/spark/rdd/RDD.scala:546: warning: The link target "PairRDDFunctions.aggregateByKey" is ambiguous. Several members fit the target:
[U](zeroValue: U)(seqOp: (U, V) => U,combOp: (U, U) => U)(implicit evidence$3: scala.reflect.ClassTag[U]): org.apache.spark.rdd.RDD[(K, U)] in class PairRDDFunctions [chosen]
[U](zeroValue: U,numPartitions: Int)(seqOp: (U, V) => U,combOp: (U, U) => U)(implicit evidence$2: scala.reflect.ClassTag[U]): org.apache.spark.rdd.RDD[(K, U)] in class PairRDDFunctions
[U](zeroValue: U,partitioner: org.apache.spark.Partitioner)(seqOp: (U, V) => U,combOp: (U, U) => U)(implicit evidence$1: scala.reflect.ClassTag[U]): org.apache.spark.rdd.RDD[(K, U)] in class PairRDDFunctions


  /**
  ^
/home/hadoop/develop/spark/core/src/main/scala/org/apache/spark/rdd/RDD.scala:1525: warning: The link target "PairRDDFunctions.reduceByKey" is ambiguous. Several members fit the target:
(func: (V, V) => V): org.apache.spark.rdd.RDD[(K, V)] in class PairRDDFunctions [chosen]
(func: (V, V) => V,numPartitions: Int): org.apache.spark.rdd.RDD[(K, V)] in class PairRDDFunctions
(partitioner: org.apache.spark.Partitioner,func: (V, V) => V): org.apache.spark.rdd.RDD[(K, V)] in class PairRDDFunctions


/**
^
/home/hadoop/develop/spark/core/src/main/scala/org/apache/spark/rdd/PairRDDFunctions.scala:556: warning: The link target "PairRDDFunctions.aggregateByKey" is ambiguous. Several members fit the target:
[U](zeroValue: U)(seqOp: (U, V) => U,combOp: (U, U) => U)(implicit evidence$3: scala.reflect.ClassTag[U]): org.apache.spark.rdd.RDD[(K, U)] in class PairRDDFunctions [chosen]
[U](zeroValue: U,numPartitions: Int)(seqOp: (U, V) => U,combOp: (U, U) => U)(implicit evidence$2: scala.reflect.ClassTag[U]): org.apache.spark.rdd.RDD[(K, U)] in class PairRDDFunctions
[U](zeroValue: U,partitioner: org.apache.spark.Partitioner)(seqOp: (U, V) => U,combOp: (U, U) => U)(implicit evidence$1: scala.reflect.ClassTag[U]): org.apache.spark.rdd.RDD[(K, U)] in class PairRDDFunctions


  /**
  ^
/home/hadoop/develop/spark/core/src/main/scala/org/apache/spark/rdd/PairRDDFunctions.scala:456: warning: The link target "PairRDDFunctions.aggregateByKey" is ambiguous. Several members fit the target:
[U](zeroValue: U)(seqOp: (U, V) => U,combOp: (U, U) => U)(implicit evidence$3: scala.reflect.ClassTag[U]): org.apache.spark.rdd.RDD[(K, U)] in class PairRDDFunctions [chosen]
[U](zeroValue: U,numPartitions: Int)(seqOp: (U, V) => U,combOp: (U, U) => U)(implicit evidence$2: scala.reflect.ClassTag[U]): org.apache.spark.rdd.RDD[(K, U)] in class PairRDDFunctions
[U](zeroValue: U,partitioner: org.apache.spark.Partitioner)(seqOp: (U, V) => U,combOp: (U, U) => U)(implicit evidence$1: scala.reflect.ClassTag[U]): org.apache.spark.rdd.RDD[(K, U)] in class PairRDDFunctions


  /**
  ^
/home/hadoop/develop/spark/core/src/main/scala/org/apache/spark/rdd/PairRDDFunctions.scala:431: warning: The link target "PairRDDFunctions.aggregateByKey" is ambiguous. Several members fit the target:
[U](zeroValue: U)(seqOp: (U, V) => U,combOp: (U, U) => U)(implicit evidence$3: scala.reflect.ClassTag[U]): org.apache.spark.rdd.RDD[(K, U)] in class PairRDDFunctions [chosen]
[U](zeroValue: U,numPartitions: Int)(seqOp: (U, V) => U,combOp: (U, U) => U)(implicit evidence$2: scala.reflect.ClassTag[U]): org.apache.spark.rdd.RDD[(K, U)] in class PairRDDFunctions
[U](zeroValue: U,partitioner: org.apache.spark.Partitioner)(seqOp: (U, V) => U,combOp: (U, U) => U)(implicit evidence$1: scala.reflect.ClassTag[U]): org.apache.spark.rdd.RDD[(K, U)] in class PairRDDFunctions


  /**
  ^
/home/hadoop/develop/spark/core/src/main/scala/org/apache/spark/rdd/NewHadoopRDD.scala:51: warning: Could not find any member to link for "org.apache.spark.SparkContext.newAPIHadoopRDD()".
/**
^
/home/hadoop/develop/spark/core/src/main/scala/org/apache/spark/rdd/HadoopRDD.scala:81: warning: Could not find any member to link for "org.apache.spark.SparkContext.hadoopRDD()".
/**
^
/home/hadoop/develop/spark/core/src/main/scala/org/apache/spark/executor/package.scala:20: warning: Could not find any member to link for "org.apache.spark.executor.Executor".
/**
^
/home/hadoop/develop/spark/core/src/main/scala/org/apache/spark/deploy/history/HistoryServer.scala:170: warning: Variable SPARK undefined in comment for object HistoryServer in object HistoryServer
object HistoryServer extends Logging {
       ^
/home/hadoop/develop/spark/core/src/main/scala/org/apache/spark/deploy/SparkHadoopUtil.scala:190: warning: Could not find any member to link for "FileStatus".
  /**
  ^
/home/hadoop/develop/spark/core/src/main/scala/org/apache/spark/broadcast/TorrentBroadcastFactory.scala:24: warning: Could not find any member to link for "org.apache.spark.broadcast.TorrentBroadcast".
/**
^
/home/hadoop/develop/spark/core/src/main/scala/org/apache/spark/broadcast/HttpBroadcastFactory.scala:24: warning: Could not find any member to link for "org.apache.spark.broadcast.HttpBroadcast".
/**
^
/home/hadoop/develop/spark/core/src/main/scala/org/apache/spark/Accumulators.scala:201: warning: The link target "SparkContext#accumulator" is ambiguous. Several members fit the target:
[T](initialValue: T,name: String)(implicit param: org.apache.spark.AccumulatorParam[T]): org.apache.spark.Accumulator[T] in class SparkContext [chosen]
[T](initialValue: T)(implicit param: org.apache.spark.AccumulatorParam[T]): org.apache.spark.Accumulator[T] in class SparkContext


/**
^
/home/hadoop/develop/spark/core/src/main/scala/org/apache/spark/api/java/JavaSparkContext.scala:797: warning: The link target "org.apache.spark.api.java.JavaSparkContext.setJobGroup" is ambiguous. Several members fit the target:
(groupId: String,description: String): Unit in class JavaSparkContext [chosen]
(groupId: String,description: String,interruptOnCancel: Boolean): Unit in class JavaSparkContext


  /**
  ^
/home/hadoop/develop/spark/core/src/main/scala/org/apache/spark/api/java/JavaRDDLike.scala:399: warning: The link target "org.apache.spark.api.java.JavaRDDLike#treeAggregate" is ambiguous. Several members fit the target:
[U](zeroValue: U,seqOp: org.apache.spark.api.java.function.Function2[U,T,U],combOp: org.apache.spark.api.java.function.Function2[U,U,U]): U in trait JavaRDDLike [chosen]
[U](zeroValue: U,seqOp: org.apache.spark.api.java.function.Function2[U,T,U],combOp: org.apache.spark.api.java.function.Function2[U,U,U],depth: Int): U in trait JavaRDDLike


  /**
  ^
/home/hadoop/develop/spark/core/src/main/scala/org/apache/spark/api/java/JavaRDDLike.scala:359: warning: The link target "org.apache.spark.api.java.JavaRDDLike#treeReduce" is ambiguous. Several members fit the target:
(f: org.apache.spark.api.java.function.Function2[T,T,T]): T in trait JavaRDDLike [chosen]
(f: org.apache.spark.api.java.function.Function2[T,T,T],depth: Int): T in trait JavaRDDLike


  /**
  ^
/home/hadoop/develop/spark/core/src/main/scala/org/apache/spark/api/java/JavaPairRDD.scala:510: warning: The link target "JavaPairRDD.reduceByKey" is ambiguous. Several members fit the target:
(func: org.apache.spark.api.java.function.Function2[V,V,V]): org.apache.spark.api.java.JavaPairRDD[K,V] in class JavaPairRDD [chosen]
(func: org.apache.spark.api.java.function.Function2[V,V,V],numPartitions: Int): org.apache.spark.api.java.JavaPairRDD[K,V] in class JavaPairRDD
(partitioner: org.apache.spark.Partitioner,func: org.apache.spark.api.java.function.Function2[V,V,V]): org.apache.spark.api.java.JavaPairRDD[K,V] in class JavaPairRDD


  /**
  ^
/home/hadoop/develop/spark/core/src/main/scala/org/apache/spark/api/java/JavaPairRDD.scala:380: warning: The link target "JavaPairRDD.reduceByKey" is ambiguous. Several members fit the target:
(func: org.apache.spark.api.java.function.Function2[V,V,V]): org.apache.spark.api.java.JavaPairRDD[K,V] in class JavaPairRDD [chosen]
(func: org.apache.spark.api.java.function.Function2[V,V,V],numPartitions: Int): org.apache.spark.api.java.JavaPairRDD[K,V] in class JavaPairRDD
(partitioner: org.apache.spark.Partitioner,func: org.apache.spark.api.java.function.Function2[V,V,V]): org.apache.spark.api.java.JavaPairRDD[K,V] in class JavaPairRDD


  /**
  ^
/home/hadoop/develop/spark/core/src/main/scala/org/apache/spark/api/java/JavaPairRDD.scala:369: warning: The link target "JavaPairRDD.reduceByKey" is ambiguous. Several members fit the target:
(func: org.apache.spark.api.java.function.Function2[V,V,V]): org.apache.spark.api.java.JavaPairRDD[K,V] in class JavaPairRDD [chosen]
(func: org.apache.spark.api.java.function.Function2[V,V,V],numPartitions: Int): org.apache.spark.api.java.JavaPairRDD[K,V] in class JavaPairRDD
(partitioner: org.apache.spark.Partitioner,func: org.apache.spark.api.java.function.Function2[V,V,V]): org.apache.spark.api.java.JavaPairRDD[K,V] in class JavaPairRDD


  /**
  ^
/home/hadoop/develop/spark/core/src/main/scala/org/apache/spark/api/java/JavaPairRDD.scala:177: warning: The link target "sampleByKey" is ambiguous. Several members fit the target:
(withReplacement: Boolean,fractions: java.util.Map[K,Double]): org.apache.spark.api.java.JavaPairRDD[K,V] in class JavaPairRDD [chosen]
(withReplacement: Boolean,fractions: java.util.Map[K,Double],seed: Long): org.apache.spark.api.java.JavaPairRDD[K,V] in class JavaPairRDD


  /**
  ^
/home/hadoop/develop/spark/core/src/main/scala/org/apache/spark/api/java/JavaPairRDD.scala:160: warning: The link target "sampleByKey" is ambiguous. Several members fit the target:
(withReplacement: Boolean,fractions: java.util.Map[K,Double]): org.apache.spark.api.java.JavaPairRDD[K,V] in class JavaPairRDD [chosen]
(withReplacement: Boolean,fractions: java.util.Map[K,Double],seed: Long): org.apache.spark.api.java.JavaPairRDD[K,V] in class JavaPairRDD


  /**
  ^
/home/hadoop/develop/spark/core/src/main/scala/org/apache/spark/network/nio/NioBlockTransferService.scala:32: warning: Could not find any member to link for "BlockTransferService".
/**
^
/home/hadoop/develop/spark/core/src/main/scala/org/apache/spark/network/BlockTransferService.scala:81: warning: The link target "init" is ambiguous. Several members fit the target:
(blockDataManager: org.apache.spark.network.BlockDataManager): Unit in class NioBlockTransferService [chosen]
(x$1: String): Unit in class NioBlockTransferService


  /**
  ^
/home/hadoop/develop/spark/core/src/main/scala/org/apache/spark/network/BlockTransferService.scala:55: warning: The link target "init" is ambiguous. Several members fit the target:
(blockDataManager: org.apache.spark.network.BlockDataManager): Unit in class NioBlockTransferService [chosen]
(x$1: String): Unit in class NioBlockTransferService


  /**
  ^
/home/hadoop/develop/spark/core/src/main/scala/org/apache/spark/network/nio/NioBlockTransferService.scala:51: warning: The link target "init" is ambiguous. Several members fit the target:
(blockDataManager: org.apache.spark.network.BlockDataManager): Unit in class NioBlockTransferService [chosen]
(x$1: String): Unit in class NioBlockTransferService


  /**
  ^
/home/hadoop/develop/spark/core/src/main/scala/org/apache/spark/network/nio/NioBlockTransferService.scala:43: warning: The link target "init" is ambiguous. Several members fit the target:
(blockDataManager: org.apache.spark.network.BlockDataManager): Unit in class NioBlockTransferService [chosen]
(x$1: String): Unit in class NioBlockTransferService


  /**
  ^
/home/hadoop/develop/spark/core/src/main/scala/org/apache/spark/network/nio/NioBlockTransferService.scala:132: warning: The link target "init" is ambiguous. Several members fit the target:
(blockDataManager: org.apache.spark.network.BlockDataManager): Unit in class NioBlockTransferService [chosen]
(x$1: String): Unit in class NioBlockTransferService


  /**
  ^
/home/hadoop/develop/spark/core/src/main/scala/org/apache/spark/network/BlockTransferService.scala:105: warning: The link target "init" is ambiguous. Several members fit the target:
(blockDataManager: org.apache.spark.network.BlockDataManager): Unit in class NioBlockTransferService [chosen]
(x$1: String): Unit in class NioBlockTransferService


  /**
  ^
/home/hadoop/develop/spark/core/src/main/scala/org/apache/spark/network/netty/SparkTransportConf.scala:41: warning: Could not find any member to link for "TransportConf".
  /**
  ^
/home/hadoop/develop/spark/core/src/main/scala/org/apache/spark/network/BlockTransferService.scala:50: warning: The link target "init" is ambiguous. Several members fit the target:
(blockDataManager: org.apache.spark.network.BlockDataManager): Unit in class NettyBlockTransferService [chosen]
(x$1: String): Unit in class NettyBlockTransferService


  /**
  ^
/home/hadoop/develop/spark/core/src/main/scala/org/apache/spark/network/BlockTransferService.scala:45: warning: The link target "init" is ambiguous. Several members fit the target:
(blockDataManager: org.apache.spark.network.BlockDataManager): Unit in class NettyBlockTransferService [chosen]
(x$1: String): Unit in class NettyBlockTransferService


  /**
  ^
/home/hadoop/develop/spark/core/src/main/scala/org/apache/spark/network/BlockTransferService.scala:70: warning: The link target "init" is ambiguous. Several members fit the target:
(blockDataManager: org.apache.spark.network.BlockDataManager): Unit in class NettyBlockTransferService [chosen]
(x$1: String): Unit in class NettyBlockTransferService


  /**
  ^
38 warnings found
[INFO] Building jar: /home/hadoop/develop/spark/core/target/spark-core_2.10-1.3.0-javadoc.jar
[INFO]  already added, skipping
[INFO] 
[INFO] --- maven-install-plugin:2.5.1:install (default-install) @ spark-core_2.10 ---
[INFO] Installing /home/hadoop/develop/spark/core/target/spark-core_2.10-1.3.0.jar to /home/hadoop/develop/m2repo/org/apache/spark/spark-core_2.10/1.3.0/spark-core_2.10-1.3.0.jar
[INFO] Installing /home/hadoop/develop/spark/core/dependency-reduced-pom.xml to /home/hadoop/develop/m2repo/org/apache/spark/spark-core_2.10/1.3.0/spark-core_2.10-1.3.0.pom
[INFO] Installing /home/hadoop/develop/spark/core/target/spark-core_2.10-1.3.0-sources.jar to /home/hadoop/develop/m2repo/org/apache/spark/spark-core_2.10/1.3.0/spark-core_2.10-1.3.0-sources.jar
[INFO] Installing /home/hadoop/develop/spark/core/target/spark-core_2.10-1.3.0-javadoc.jar to /home/hadoop/develop/m2repo/org/apache/spark/spark-core_2.10/1.3.0/spark-core_2.10-1.3.0-javadoc.jar
[INFO] ------------------------------------------------------------------------
[INFO] BUILD SUCCESS
[INFO] ------------------------------------------------------------------------
[INFO] Total time: 6:22.194s
[INFO] Finished at: Tue Aug 11 10:14:41 CST 2015
[INFO] Final Memory: 47M/719M
[INFO] ------------------------------------------------------------------------
